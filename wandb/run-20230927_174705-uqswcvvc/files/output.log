Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/38150 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.





















































  1%|▏         | 506/38150 [01:46<2:15:50,  4.62it/s]



























  2%|▏         | 762/38150 [02:41<2:15:41,  4.59it/s]



100%|██████████| 95/95 [00:05<00:00, 15.03it/s]






