Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Map:  76%|███████▌  | 37000/48798 [00:01<00:00, 16977.09 examples/s]
  0%|          | 0/38150 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
















































  1%|▏         | 506/38150 [01:38<2:02:55,  5.10it/s]

























  2%|▏         | 762/38150 [02:29<2:03:57,  5.03it/s]


 99%|█████████▉| 94/95 [00:04<00:00, 14.94it/s]
























  3%|▎         | 991/38150 [03:31<2:06:08,  4.91it/s]

















































  4%|▍         | 1496/38150 [05:11<2:02:47,  4.98it/s]


  4%|▍         | 1525/38150 [05:17<2:12:43,  4.60it/s]



 95%|█████████▍| 90/95 [00:04<00:00, 16.84it/s]















































  5%|▌         | 1994/38150 [07:01<2:02:34,  4.92it/s]





























  6%|▌         | 2288/38150 [08:00<1:55:55,  5.16it/s]


 99%|█████████▉| 94/95 [00:04<00:00, 14.87it/s]





















  7%|▋         | 2505/38150 [08:54<1:50:24,  5.38it/s]

















































  8%|▊         | 3001/38150 [10:33<1:49:36,  5.34it/s]




  8%|▊         | 3052/38150 [10:43<1:43:35,  5.65it/s]



 88%|████████▊ | 84/95 [00:03<00:00, 16.55it/s]

  8%|▊         | 3052/38150 [10:55<2:05:37,  4.66it/s]
{'train_runtime': 655.4366, 'train_samples_per_second': 3722.557, 'train_steps_per_second': 58.205, 'train_loss': 0.2101999527504997, 'epoch': 4.0}