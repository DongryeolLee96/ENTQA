Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/246 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.


















100%|█████████▉| 245/246 [00:39<00:00,  6.03it/s]

 45%|████▌     | 14/31 [00:00<00:00, 24.42it/s]

100%|██████████| 246/246 [00:47<00:00,  5.17it/s]
{'train_runtime': 47.5506, 'train_samples_per_second': 330.469, 'train_steps_per_second': 5.173, 'train_loss': 0.4681099992457444, 'epoch': 1.0}