Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Map:  64%|██████▍   | 36000/56104 [00:01<00:01, 15656.05 examples/s]
  0%|          | 0/43850 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.














































  1%|          | 504/43850 [01:33<2:18:08,  5.23it/s]



































  2%|▏         | 877/43850 [02:45<2:09:22,  5.54it/s]



100%|██████████| 95/95 [00:04<00:00, 16.07it/s]














  2%|▏         | 1005/43850 [03:21<2:26:34,  4.87it/s]
















































  3%|▎         | 1506/43850 [04:58<2:15:41,  5.20it/s]























  4%|▍         | 1753/43850 [05:46<2:14:12,  5.23it/s]



 92%|█████████▏| 87/95 [00:04<00:00, 17.31it/s]

























  5%|▍         | 2008/43850 [06:46<2:04:41,  5.59it/s]
















































  6%|▌         | 2505/43850 [08:22<2:08:08,  5.38it/s]











  6%|▌         | 2631/43850 [08:45<1:52:19,  6.12it/s]



 94%|█████████▎| 89/95 [00:04<00:00, 17.10it/s]

  6%|▌         | 2631/43850 [08:57<2:20:19,  4.90it/s]
{'train_runtime': 537.4444, 'train_samples_per_second': 5219.517, 'train_steps_per_second': 81.59, 'train_loss': 0.2191972330195602, 'epoch': 3.0}