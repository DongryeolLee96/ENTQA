Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Map:  57%|█████▋    | 36000/63410 [00:01<00:01, 16626.49 examples/s]
  0%|          | 0/49550 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.















































  1%|          | 500/49550 [01:35<2:37:08,  5.20it/s]














































  2%|▏         | 991/49550 [03:08<2:27:24,  5.49it/s]


100%|██████████| 95/95 [00:04<00:00, 16.01it/s]


  2%|▏         | 997/49550 [03:25<13:30:56,  1.00s/it]
















































  3%|▎         | 1498/49550 [05:01<2:23:05,  5.60it/s]














































  4%|▍         | 1982/49550 [06:34<2:24:59,  5.47it/s]



100%|██████████| 95/95 [00:04<00:00, 15.94it/s]


  4%|▍         | 2001/49550 [06:53<2:43:25,  4.85it/s]
















































  5%|▌         | 2504/49550 [08:30<2:21:45,  5.53it/s]













































  6%|▌         | 2973/49550 [10:01<2:11:40,  5.90it/s]



100%|██████████| 95/95 [00:04<00:00, 15.92it/s]
{'eval_loss': 0.30879276990890503, 'eval_accuracy': 0.858345715700842, 'eval_runtime': 6.4641, 'eval_samples_per_second': 937.018, 'eval_steps_per_second': 14.697, 'epoch': 3.0}

  6%|▌         | 2973/49550 [10:12<2:40:01,  4.85it/s]