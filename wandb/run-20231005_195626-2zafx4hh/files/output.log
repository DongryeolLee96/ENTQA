Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/43850 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.


































  1%|          | 500/43850 [01:08<1:39:26,  7.27it/s]

















  2%|▏         | 876/43850 [02:02<1:56:59,  6.12it/s]


 98%|█████████▊| 93/95 [00:02<00:00, 24.37it/s]









  2%|▏         | 1004/43850 [02:30<1:57:39,  6.07it/s]

































  3%|▎         | 1501/43850 [03:41<1:35:32,  7.39it/s]


















  4%|▍         | 1753/43850 [04:18<1:39:59,  7.02it/s]


 98%|█████████▊| 93/95 [00:02<00:00, 24.30it/s]

















  5%|▍         | 1997/43850 [05:02<1:35:42,  7.29it/s]





























  6%|▌         | 2500/43850 [06:15<1:39:30,  6.93it/s]









  6%|▌         | 2630/43850 [06:33<1:32:10,  7.45it/s]

 98%|█████████▊| 93/95 [00:02<00:00, 24.29it/s]

{'eval_loss': 0.5589584708213806, 'eval_accuracy': 0.7302294865444939, 'eval_runtime': 4.3124, 'eval_samples_per_second': 1404.563, 'eval_steps_per_second': 22.03, 'epoch': 3.0}

  6%|▌         | 2631/43850 [06:43<1:45:18,  6.52it/s]