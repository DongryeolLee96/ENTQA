Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/143 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.















 99%|█████████▉| 142/143 [00:33<00:00,  3.86it/s]



 99%|█████████▊| 69/70 [00:04<00:00, 16.56it/s]

100%|██████████| 143/143 [00:45<00:00,  3.17it/s]
{'train_runtime': 45.1708, 'train_samples_per_second': 201.236, 'train_steps_per_second': 3.166, 'train_loss': 0.37730669141649364, 'epoch': 1.0}