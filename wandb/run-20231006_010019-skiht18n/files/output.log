Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/143 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.














 99%|█████████▉| 142/143 [00:30<00:00,  4.37it/s]


 86%|████████▌ | 60/70 [00:03<00:00, 16.07it/s]
{'eval_loss': 0.33934471011161804, 'eval_accuracy': 0.8675213675213675, 'eval_runtime': 5.2553, 'eval_samples_per_second': 845.998, 'eval_steps_per_second': 13.32, 'epoch': 1.0}

100%|██████████| 143/143 [00:40<00:00,  3.49it/s]