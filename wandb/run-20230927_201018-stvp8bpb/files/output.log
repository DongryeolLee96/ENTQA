Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/20200 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.








































  2%|▏         | 404/20200 [01:22<1:03:28,  5.20it/s]


100%|██████████| 51/51 [00:02<00:00, 16.26it/s]










  2%|▏         | 503/20200 [01:51<1:09:14,  4.74it/s]































  4%|▍         | 808/20200 [02:54<59:48,  5.40it/s]


100%|██████████| 51/51 [00:02<00:00, 16.05it/s]




















  5%|▍         | 1001/20200 [03:43<1:11:25,  4.48it/s]





















  6%|▌         | 1212/20200 [04:26<58:44,  5.39it/s]


 94%|█████████▍| 48/51 [00:02<00:00, 16.66it/s]






























  7%|▋         | 1501/20200 [05:35<1:05:54,  4.73it/s]











  8%|▊         | 1616/20200 [05:59<56:16,  5.50it/s]


100%|██████████| 51/51 [00:02<00:00, 16.27it/s]
{'eval_loss': 0.23617620766162872, 'eval_accuracy': 0.9160686427457099, 'eval_runtime': 14.2187, 'eval_samples_per_second': 225.407, 'eval_steps_per_second': 3.587, 'epoch': 4.0}

  8%|▊         | 1616/20200 [06:18<1:12:31,  4.27it/s]