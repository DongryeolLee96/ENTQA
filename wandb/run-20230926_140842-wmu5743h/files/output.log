Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/20200 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.








































  2%|▏         | 404/20200 [01:21<1:02:58,  5.24it/s]


 95%|█████████▌| 59/62 [00:03<00:00, 16.44it/s]










  2%|▏         | 504/20200 [01:51<1:10:21,  4.67it/s]































  4%|▍         | 808/20200 [02:54<59:24,  5.44it/s]


 95%|█████████▌| 59/62 [00:03<00:00, 16.27it/s]




















  5%|▍         | 998/20200 [03:44<1:11:02,  4.50it/s]





















  6%|▌         | 1212/20200 [04:28<58:10,  5.44it/s]



 98%|█████████▊| 61/62 [00:03<00:00, 14.24it/s]

  6%|▌         | 1212/20200 [04:38<1:12:37,  4.36it/s]
{'train_runtime': 278.1219, 'train_samples_per_second': 4643.826, 'train_steps_per_second': 72.63, 'train_loss': 0.2979989004607248, 'epoch': 3.0}