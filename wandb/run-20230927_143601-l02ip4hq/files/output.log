Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/7150 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.















  2%|▏         | 142/7150 [00:33<29:29,  3.96it/s]


 83%|████████▎ | 58/70 [00:03<00:00, 15.77it/s]

















  4%|▍         | 285/7150 [01:18<30:41,  3.73it/s]


 99%|█████████▊| 69/70 [00:04<00:00, 16.46it/s]


















  6%|▌         | 428/7150 [02:02<30:33,  3.67it/s]

 81%|████████▏ | 57/70 [00:03<00:00, 15.01it/s]


 99%|█████████▊| 69/70 [00:04<00:00, 16.34it/s]

  6%|▌         | 429/7150 [02:14<35:08,  3.19it/s]