Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Map:  46%|████▌     | 29000/63410 [00:01<00:01, 21293.84 examples/s]
  0%|          | 0/991 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.















































 51%|█████     | 503/991 [01:35<01:32,  5.28it/s]














































100%|██████████| 991/991 [03:08<00:00,  5.45it/s]


 96%|█████████▌| 43/45 [00:01<00:00, 21.81it/s]
{'eval_loss': 0.1843305379152298, 'eval_accuracy': 0.9274193548387096, 'eval_runtime': 8.1588, 'eval_samples_per_second': 349.561, 'eval_steps_per_second': 5.516, 'epoch': 1.0}

100%|██████████| 991/991 [03:22<00:00,  4.90it/s]