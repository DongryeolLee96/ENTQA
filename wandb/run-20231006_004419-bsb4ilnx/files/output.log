Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/143 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.















 99%|█████████▉| 142/143 [00:30<00:00,  4.31it/s]


 94%|█████████▍| 66/70 [00:03<00:00, 16.91it/s]
{'eval_loss': 0.38241690397262573, 'eval_accuracy': 0.8596491228070176, 'eval_runtime': 5.3295, 'eval_samples_per_second': 834.227, 'eval_steps_per_second': 13.134, 'epoch': 1.0}

100%|██████████| 143/143 [00:41<00:00,  3.45it/s]