Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/7150 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.














  2%|▏         | 142/7150 [00:30<26:51,  4.35it/s]


 86%|████████▌ | 60/70 [00:03<00:00, 16.20it/s]















  4%|▍         | 285/7150 [01:11<26:16,  4.35it/s]


 99%|█████████▊| 69/70 [00:03<00:00, 18.29it/s]
















  6%|▌         | 428/7150 [01:51<26:32,  4.22it/s]


 99%|█████████▊| 69/70 [00:03<00:00, 18.19it/s]









  7%|▋         | 496/7150 [02:16<23:14,  4.77it/s]








  8%|▊         | 571/7150 [02:32<23:24,  4.68it/s]


 80%|████████  | 56/70 [00:03<00:00, 16.67it/s]
{'eval_loss': 0.5029714107513428, 'eval_accuracy': 0.8531264057579847, 'eval_runtime': 5.3165, 'eval_samples_per_second': 836.26, 'eval_steps_per_second': 13.166, 'epoch': 4.0}

  8%|▊         | 572/7150 [02:43<31:19,  3.50it/s]