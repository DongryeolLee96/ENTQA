Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/20200 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.







































  2%|▏         | 404/20200 [01:20<1:02:02,  5.32it/s]


 98%|█████████▊| 50/51 [00:02<00:00, 15.09it/s]










  2%|▏         | 503/20200 [01:49<1:08:09,  4.82it/s]































  4%|▍         | 808/20200 [02:52<58:35,  5.52it/s]

 98%|█████████▊| 50/51 [00:02<00:00, 14.96it/s]






















  5%|▍         | 1005/20200 [03:41<1:05:15,  4.90it/s]




















  6%|▌         | 1212/20200 [04:23<57:36,  5.49it/s]

 98%|█████████▊| 50/51 [00:02<00:00, 14.97it/s]































  7%|▋         | 1500/20200 [05:31<1:02:52,  4.96it/s]











  8%|▊         | 1616/20200 [05:54<55:34,  5.57it/s]


 98%|█████████▊| 50/51 [00:02<00:00, 14.98it/s]
{'eval_loss': 0.23617620766162872, 'eval_accuracy': 0.9160686427457099, 'eval_runtime': 3.9094, 'eval_samples_per_second': 819.809, 'eval_steps_per_second': 13.045, 'epoch': 4.0}

  8%|▊         | 1616/20200 [06:03<1:09:45,  4.44it/s]