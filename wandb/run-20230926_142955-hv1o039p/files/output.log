Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/20200 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.





























  2%|▏         | 404/20200 [01:00<46:53,  7.04it/s]

 69%|██████▊   | 35/51 [00:01<00:00, 25.93it/s]







  2%|▏         | 496/20200 [01:22<50:22,  6.52it/s]























  4%|▍         | 808/20200 [02:09<44:18,  7.29it/s]

 96%|█████████▌| 49/51 [00:01<00:00, 21.84it/s]
















  5%|▍         | 995/20200 [02:46<53:49,  5.95it/s]
















  6%|▌         | 1211/20200 [03:19<47:58,  6.60it/s]

 96%|█████████▌| 49/51 [00:01<00:00, 21.67it/s]























  7%|▋         | 1494/20200 [04:10<48:28,  6.43it/s]









  8%|▊         | 1615/20200 [04:28<46:27,  6.67it/s]

 78%|███████▊  | 40/51 [00:01<00:00, 22.68it/s]
{'eval_loss': 0.24243314564228058, 'eval_accuracy': 0.9216848673946958, 'eval_runtime': 3.342, 'eval_samples_per_second': 958.994, 'eval_steps_per_second': 15.26, 'epoch': 4.0}

  8%|▊         | 1616/20200 [04:36<53:01,  5.84it/s]