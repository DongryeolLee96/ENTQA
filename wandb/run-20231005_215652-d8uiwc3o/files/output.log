Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/12300 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.


















  2%|▏         | 245/12300 [00:39<33:10,  6.06it/s]

 45%|████▌     | 14/31 [00:00<00:00, 24.34it/s]




















  4%|▍         | 491/12300 [01:26<31:07,  6.32it/s]

 74%|███████▍  | 23/31 [00:01<00:00, 19.69it/s]
{'eval_loss': 0.22478258609771729, 'eval_accuracy': 0.9116445352400409, 'eval_runtime': 2.6353, 'eval_samples_per_second': 742.981, 'eval_steps_per_second': 11.763, 'epoch': 2.0}
















