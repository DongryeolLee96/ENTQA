Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/12300 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.


















  2%|▏         | 245/12300 [00:39<33:41,  5.96it/s]


 94%|█████████▎| 29/31 [00:01<00:00, 20.66it/s]





















  4%|▍         | 491/12300 [01:28<31:07,  6.32it/s]

100%|██████████| 31/31 [00:01<00:00, 21.57it/s]

  4%|▍         | 503/12300 [01:38<43:31,  4.52it/s]



















  6%|▌         | 737/12300 [02:17<31:55,  6.04it/s]

 71%|███████   | 22/31 [00:01<00:00, 19.15it/s]




















  8%|▊         | 983/12300 [03:05<30:41,  6.14it/s]


100%|██████████| 31/31 [00:01<00:00, 21.47it/s]
{'eval_loss': 0.2728065252304077, 'eval_accuracy': 0.9121552604698672, 'eval_runtime': 2.6766, 'eval_samples_per_second': 731.512, 'eval_steps_per_second': 11.582, 'epoch': 4.0}

  8%|▊         | 984/12300 [03:13<37:10,  5.07it/s]