Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

  0%|          | 0/38150 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.















































  1%|▏         | 505/38150 [01:36<2:02:14,  5.13it/s]
























  2%|▏         | 762/38150 [02:25<2:00:05,  5.19it/s]



100%|██████████| 95/95 [00:04<00:00, 16.33it/s]
























  3%|▎         | 1008/38150 [03:22<1:44:01,  5.95it/s]















































  4%|▍         | 1493/38150 [04:56<2:02:04,  5.00it/s]


  4%|▍         | 1525/38150 [05:02<2:08:36,  4.75it/s]


 92%|█████████▏| 87/95 [00:03<00:00, 17.64it/s]














































  5%|▌         | 1992/38150 [06:43<1:52:17,  5.37it/s]




























  6%|▌         | 2288/38150 [07:40<1:53:15,  5.28it/s]


100%|██████████| 95/95 [00:04<00:00, 16.20it/s]





















  7%|▋         | 2497/38150 [08:30<2:02:03,  4.87it/s]














































  8%|▊         | 3000/38150 [10:08<1:45:28,  5.55it/s]




  8%|▊         | 3052/38150 [10:18<1:40:42,  5.81it/s]



 94%|█████████▎| 89/95 [00:04<00:00, 17.33it/s]

  8%|▊         | 3052/38150 [10:29<2:00:43,  4.85it/s]
{'train_runtime': 629.8585, 'train_samples_per_second': 3873.727, 'train_steps_per_second': 60.569, 'train_loss': 0.2101999527504997, 'epoch': 4.0}