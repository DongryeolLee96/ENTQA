Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/7150 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
















  2%|▏         | 142/7150 [00:33<29:53,  3.91it/s]


 99%|█████████▊| 69/70 [00:04<00:00, 16.60it/s]


















  4%|▍         | 285/7150 [01:17<30:31,  3.75it/s]


 99%|█████████▊| 69/70 [00:04<00:00, 16.50it/s]


















  6%|▌         | 428/7150 [02:02<30:21,  3.69it/s]


 99%|█████████▊| 69/70 [00:04<00:00, 16.40it/s]

{'eval_loss': 0.8368626832962036, 'eval_accuracy': 0.45996401259559155, 'eval_runtime': 6.7516, 'eval_samples_per_second': 658.506, 'eval_steps_per_second': 10.368, 'epoch': 3.0}

  6%|▌         | 429/7150 [02:15<35:23,  3.16it/s]