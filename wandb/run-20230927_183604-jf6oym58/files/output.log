Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/143 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.














 99%|█████████▉| 142/143 [00:30<00:00,  4.32it/s]


 99%|█████████▊| 69/70 [00:03<00:00, 18.01it/s]

{'eval_loss': 0.3437274396419525, 'eval_accuracy': 0.8672964462438146, 'eval_runtime': 5.7119, 'eval_samples_per_second': 778.38, 'eval_steps_per_second': 12.255, 'epoch': 1.0}

100%|██████████| 143/143 [00:41<00:00,  3.41it/s]