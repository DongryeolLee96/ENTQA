Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/20100 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.































  2%|▏         | 402/20100 [01:04<47:56,  6.85it/s]


100%|██████████| 50/50 [00:02<00:00, 19.15it/s]








  2%|▏         | 502/20100 [01:29<48:35,  6.72it/s]
























  4%|▍         | 804/20100 [02:19<50:48,  6.33it/s]


100%|██████████| 50/50 [00:02<00:00, 19.00it/s]
















  5%|▍         | 1002/20100 [02:59<48:28,  6.57it/s]
















  6%|▌         | 1206/20100 [03:33<50:16,  6.26it/s]


 98%|█████████▊| 49/50 [00:02<00:00, 20.52it/s]
























  7%|▋         | 1492/20100 [04:28<50:40,  6.12it/s]









  8%|▊         | 1608/20100 [04:47<48:48,  6.31it/s]

 94%|█████████▍| 47/50 [00:01<00:00, 21.31it/s]

































 10%|▉         | 2005/20100 [06:01<53:19,  5.66it/s]
 10%|█         | 2010/20100 [06:02<49:39,  6.07it/s]

 98%|█████████▊| 49/50 [00:02<00:00, 20.37it/s]


 10%|█         | 2010/20100 [06:11<55:39,  5.42it/s]
{'train_runtime': 371.0648, 'train_samples_per_second': 3466.107, 'train_steps_per_second': 54.168, 'train_loss': 0.23188462310762548, 'epoch': 5.0}