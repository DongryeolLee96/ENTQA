Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/20200 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.











































  2%|▏         | 404/20200 [01:30<1:10:00,  4.71it/s]


 95%|█████████▌| 59/62 [00:03<00:00, 15.24it/s]











  2%|▏         | 497/20200 [02:01<1:13:40,  4.46it/s]



































  4%|▍         | 808/20200 [03:13<1:05:44,  4.92it/s]


 95%|█████████▌| 59/62 [00:03<00:00, 15.02it/s]
























  5%|▍         | 1002/20200 [04:09<1:19:47,  4.01it/s]























  6%|▌         | 1212/20200 [04:57<1:05:52,  4.80it/s]


 98%|█████████▊| 61/62 [00:03<00:00, 13.27it/s]



































  7%|▋         | 1502/20200 [06:17<1:10:30,  4.42it/s]












  8%|▊         | 1616/20200 [06:43<1:03:07,  4.91it/s]


 98%|█████████▊| 61/62 [00:03<00:00, 13.31it/s]


  8%|▊         | 1616/20200 [06:55<1:19:34,  3.89it/s]
{'train_runtime': 415.1609, 'train_samples_per_second': 3110.962, 'train_steps_per_second': 48.656, 'train_loss': 0.3240085993662919, 'epoch': 4.0}