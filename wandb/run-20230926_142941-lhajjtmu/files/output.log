Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/20200 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.












































  2%|▏         | 404/20200 [01:29<1:09:22,  4.76it/s]


100%|██████████| 51/51 [00:03<00:00, 15.15it/s]











  2%|▏         | 500/20200 [02:01<1:13:56,  4.44it/s]



































  4%|▍         | 808/20200 [03:11<1:04:43,  4.99it/s]

 98%|█████████▊| 50/51 [00:03<00:00, 13.36it/s]























  5%|▍         | 1001/20200 [04:05<1:19:03,  4.05it/s]























  6%|▌         | 1212/20200 [04:53<1:04:37,  4.90it/s]


 78%|███████▊  | 40/51 [00:02<00:00, 14.54it/s]


































  7%|▋         | 1504/20200 [06:09<1:10:33,  4.42it/s]












  8%|▊         | 1616/20200 [06:34<1:02:08,  4.98it/s]


 94%|█████████▍| 48/51 [00:02<00:00, 15.21it/s]













































 10%|▉         | 2005/20200 [08:13<1:07:50,  4.47it/s]

 10%|█         | 2020/20200 [08:17<1:01:11,  4.95it/s]


 94%|█████████▍| 48/51 [00:02<00:00, 15.22it/s]

 10%|█         | 2020/20200 [08:27<1:16:08,  3.98it/s]
{'train_runtime': 507.5758, 'train_samples_per_second': 2544.546, 'train_steps_per_second': 39.797, 'train_loss': 0.30995807458858676, 'epoch': 5.0}