Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Map:  55%|█████▌    | 27000/48798 [00:01<00:01, 19700.30 examples/s]
  0%|          | 0/38150 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
















































  1%|▏         | 508/38150 [01:37<1:55:12,  5.45it/s]
























  2%|▏         | 762/38150 [02:27<2:02:15,  5.10it/s]


 98%|█████████▊| 93/95 [00:04<00:00, 16.79it/s]
























  3%|▎         | 1000/38150 [03:24<1:51:54,  5.53it/s]
















































  4%|▍         | 1494/38150 [05:00<2:03:01,  4.97it/s]


  4%|▍         | 1525/38150 [05:06<2:09:14,  4.72it/s]



 92%|█████████▏| 87/95 [00:04<00:00, 17.36it/s]














































  5%|▌         | 1993/38150 [06:48<1:53:35,  5.31it/s]




























  6%|▌         | 2288/38150 [07:45<1:52:26,  5.32it/s]


100%|██████████| 95/95 [00:04<00:00, 16.07it/s]





















  7%|▋         | 2497/38150 [08:36<2:01:19,  4.90it/s]

















































  8%|▊         | 2997/38150 [10:14<2:03:49,  4.73it/s]





  8%|▊         | 3052/38150 [10:24<1:41:23,  5.77it/s]


 99%|█████████▉| 94/95 [00:04<00:00, 15.26it/s]

{'eval_loss': 0.25626999139785767, 'eval_accuracy': 0.910516757470695, 'eval_runtime': 11.0505, 'eval_samples_per_second': 548.119, 'eval_steps_per_second': 8.597, 'epoch': 4.0}

  8%|▊         | 3052/38150 [10:41<2:02:52,  4.76it/s]