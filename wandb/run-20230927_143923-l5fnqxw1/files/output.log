Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/7150 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.















  2%|▏         | 142/7150 [00:33<29:55,  3.90it/s]



 97%|█████████▋| 68/70 [00:04<00:00, 16.42it/s]

















  4%|▍         | 285/7150 [01:17<30:41,  3.73it/s]



 99%|█████████▊| 69/70 [00:04<00:00, 16.48it/s]
















  6%|▌         | 428/7150 [02:02<30:24,  3.68it/s]



 99%|█████████▊| 69/70 [00:04<00:00, 16.42it/s]

  6%|▌         | 429/7150 [02:13<34:58,  3.20it/s]
{'train_runtime': 133.9771, 'train_samples_per_second': 3392.37, 'train_steps_per_second': 53.367, 'train_loss': 0.6182140714916594, 'epoch': 3.0}