{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c20638b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donaldo9603/.conda/envs/numeric/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f8dd44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "036844d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dir='/home/donaldo9603/workspace/numeric/data/ours/numeric_dev.json'\n",
    "test_dir='/home/donaldo9603/workspace/numeric/data/ours/numeric_test.json'\n",
    "train_dir='/home/donaldo9603/workspace/numeric/data/ours/numeric_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa3da799",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dev_dir) as f:\n",
    "    dev=json.load(f)\n",
    "with open(test_dir) as f:\n",
    "    test=json.load(f)\n",
    "with open(train_dir) as f:\n",
    "    train=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "108d5d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dir='/home/donaldo9603/workspace/numeric/data/ours/numeric_dev.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64391004",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev=[]\n",
    "with jsonlines.open(dev_dir) as f:\n",
    "    for line in f:\n",
    "        dev.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e59c7ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bem_model_dir='/home/donaldo9603/workspace/BEM/checkpoint/checkpoint-32443/checkpoint-143'\n",
    "numeric_model_dir='/home/donaldo9603/workspace/BEM/checkpoint/checkpoint-32594/checkpoint-359'\n",
    "numeric_robert_dir='/home/donaldo9603/workspace/BEM/checkpoint/checkpoint-32595/checkpoint-359'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aadd6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {1: \"Yes\", 0: \"No\"}\n",
    "label2id = {'Yes':1, \"No\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1826003",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer=AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b193d353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'when did the florin cease to be legal tender',\n",
       " 'reference': '30 June 1993',\n",
       " 'candidate': '30/06/1993',\n",
       " 'score': 1.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca9526c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0d8735c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4193f6",
   "metadata": {},
   "source": [
    "### numeric_BERT_model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1adcbca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(numeric_model_dir, num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9719198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9260168302945302\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "cnt=0\n",
    "for d in dev:\n",
    "    text='{} [SEP] {} [SEP] {}'.format(d['candidate'], d['reference'], d['question'])\n",
    "    inputs = bert_tokenizer(text, return_tensors=\"pt\")\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    inputs=inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits= model(**inputs).logits                    \n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    \n",
    "    if predicted_class_id == int(d['score']):\n",
    "        cnt+=1\n",
    "\n",
    "print(cnt/len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ace0078e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'when did the florin cease to be legal tender',\n",
       " 'reference': '30 June 1993',\n",
       " 'candidate': '30/06/1993',\n",
       " 'score': 1.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe45ad",
   "metadata": {},
   "source": [
    "### numeric_RoBERTa_model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a8f14b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(numeric_robert_dir, num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d9e7a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9393408134642356\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "cnt=0\n",
    "for d in dev:\n",
    "    text='{} [SEP] {} [SEP] {}'.format(d['candidate'], d['reference'], d['question'])\n",
    "    inputs = roberta_tokenizer(text, return_tensors=\"pt\")\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    inputs=inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits= model(**inputs).logits                    \n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    \n",
    "    if predicted_class_id == int(d['score']):\n",
    "        cnt+=1\n",
    "\n",
    "print(cnt/len(dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9d03f6",
   "metadata": {},
   "source": [
    "###   BEM_model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c889a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(bem_model_dir, num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "207e7732",
   "metadata": {},
   "outputs": [],
   "source": [
    "bem_wrong_case=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c27c9d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5448807854137447\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "cnt=0\n",
    "for d in dev:\n",
    "    text='{} [SEP] {} [SEP] {}'.format(d['candidate'], d['reference'], d['question'])\n",
    "    inputs = bert_tokenizer(text, return_tensors=\"pt\")\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    inputs=inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits= model(**inputs).logits                    \n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    \n",
    "    if predicted_class_id == int(d['score']):\n",
    "        cnt+=1\n",
    "        bem_wrong_case.append(d)\n",
    "print(cnt/len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "055a4293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'when did the us enter the vietnam war start',\n",
       " 'reference': '1955',\n",
       " 'candidate': 'in 1955',\n",
       " 'score': 1.0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bem_wrong_case[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f14fb8",
   "metadata": {},
   "source": [
    "### Minwoo BEM model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcd8d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"Yes\", 1: \"No\"}\n",
    "label2id = {'Yes':0, \"No\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cab77d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"/home/minwoolee/workspace/colloquial-fever/t0-q2/out/ae-30574\", num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52e636f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/minwoolee/workspace/colloquial-fever/t0-q2/out/ae-30574\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c456267",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67b761b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6195652173913043\n"
     ]
    }
   ],
   "source": [
    "bem_wrong_case=[]\n",
    "model.to(device)\n",
    "cnt=0\n",
    "for d in dev:\n",
    "    text='{} [SEP] {} [SEP] {}'.format(d['candidate'], d['reference'], d['question'])\n",
    "    inputs =tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs=inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    \n",
    "    if predicted_class_id != int(d['score']):\n",
    "        cnt+=1\n",
    "        bem_wrong_case.append(d)\n",
    "print(cnt/len(dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d5bee9",
   "metadata": {},
   "source": [
    "### 150 samples labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "63be402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import bem_score, num_score, normalize_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d4037df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample150_dir='/home/donaldo9603/workspace/numeric/data/squad/sq_num_150_zero_human_final.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "06884438",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sample150_dir) as f:\n",
    "    sq_sample=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2cc05325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How many will the host committee dedicate to local charities?',\n",
       " 'answers': ['25 percent', '25 percent of all money', '25'],\n",
       " 'ans_type': 'PERCENT',\n",
       " 'prediction': '\\n\\nThe exact amount the host committee will dedicate to local charities cannot be determined without additional information, such as the budget for the event or the total amount of funds raised.',\n",
       " 'bem': 1,\n",
       " 'em': 0,\n",
       " 'human': 0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b5efb59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in sq_sample:\n",
    "    bem=bem_score(d['prediction'], d['answers'], d['question'])\n",
    "    num_ans=num_ans_score(d['prediction'], d['answers'], d['question'])\n",
    "    num_sen=num_sen_score(d['prediction'], d['answers'], d['question'])\n",
    "    em=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answers'])\n",
    "    d['bem']=bem\n",
    "    d['num_ans']=num_ans\n",
    "    d['num_sen']=num_sen\n",
    "    d['em']=em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a207da1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: 0.200\n",
      "BEM: 0.773\n",
      "Human: 0.267\n",
      "EM-recall based on Human label: 0.675\n"
     ]
    }
   ],
   "source": [
    "bem_sum=0\n",
    "em_sum=0\n",
    "human_sum=0\n",
    "em_human_rec=0\n",
    "\n",
    "for d in sq_sample:\n",
    "    bem_sum+=d['bem']\n",
    "    em_sum+=d['em']\n",
    "    human_sum+=d['human']\n",
    "    if d['human']==1 and d['em']==1:\n",
    "        em_human_rec+=1\n",
    "print('EM: %5.3f' %(em_sum/len(sq_sample)))\n",
    "print('BEM: %5.3f' %(bem_sum/len(sq_sample)))\n",
    "\n",
    "print('Human: %5.3f' %(human_sum/len(sq_sample)))\n",
    "print('EM-recall based on Human label: %5.3f' % (em_human_rec/human_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5bf04738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQUAD 150 Samples InstGPT zero-shot\n",
      "Human accuracy:  0.27\n",
      "BEM accuracy:  0.77\n",
      "NEM_ans accuracy:  0.64\n",
      "NEM_sen accuracy:  0.57\n",
      "\n",
      "BEM accuracy against human label:  0.47\n",
      "NEM_ans accuracy against human label:  0.55\n",
      "NEM_sen accuracy against human label:  0.62\n"
     ]
    }
   ],
   "source": [
    "bem_human=0\n",
    "num_ans_human=0\n",
    "num_sen_human=0\n",
    "human_sum=0\n",
    "bem_sum=0\n",
    "num_ans_sum=0\n",
    "num_sen_sum=0\n",
    "\n",
    "for d in sq_sample:\n",
    "    if d['bem']==d['human']:\n",
    "        bem_human+=1\n",
    "    if d['num_ans']==d['human']:\n",
    "        num_ans_human+=1\n",
    "    if d['num_sen']==d['human']:\n",
    "        num_sen_human+=1\n",
    "    human_sum+=d['human']\n",
    "    bem_sum+=d['bem']\n",
    "    num_ans_sum+=d['num_ans']\n",
    "    num_sen_sum+=d['num_sen']\n",
    "\n",
    "print('SQUAD 150 Samples InstGPT zero-shot')\n",
    "print('Human accuracy: %5.2f' %(human_sum/150))\n",
    "print('BEM accuracy: %5.2f'%(bem_sum/150))\n",
    "print('NEM_ans accuracy: %5.2f'%(num_ans_sum/150))\n",
    "print('NEM_sen accuracy: %5.2f'%(num_sen_sum/150))\n",
    "print()\n",
    "print('BEM accuracy against human label: %5.2f'%(bem_human/150))\n",
    "print('NEM_ans accuracy against human label: %5.2f'%(num_ans_human/150))\n",
    "print('NEM_sen accuracy against human label: %5.2f'%(num_sen_human/150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0ef3f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample150_dir='/home/donaldo9603/workspace/numeric/data/nq/nq_num_150_zero_human_final.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3da40a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sample150_dir) as f:\n",
    "    nq_sample=json.load(f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97f9b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sample150_dir, 'w') as f:\n",
    "    json.dump(nq_sample,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "615b4eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in nq_sample:\n",
    "    bem=bem_score(d['prediction'], d['answer'], d['question'])\n",
    "    num_ans=num_ans_score(d['prediction'], d['answer'], d['question'])\n",
    "    num_sen=num_sen_score(d['prediction'], d['answer'], d['question'])\n",
    "    em=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answer'])\n",
    "    d['bem']=bem\n",
    "    d['num_ans']=num_ans\n",
    "    d['num_sen']=num_sen\n",
    "    d['em']=em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ffc9904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what was the population of the roman empire at its height',\n",
       " 'answer': ['50 to 90\\xa0million inhabitants',\n",
       "  '70\\xa0million',\n",
       "  '55–60 million',\n",
       "  'an estimated 70\\xa0million people'],\n",
       " 'ans_type': 'CARDINAL',\n",
       " 'prediction': '\\n\\nThe population of the Roman Empire at its height is estimated to have been between 50 and 80 million people.',\n",
       " 'bem': 1,\n",
       " 'em': 0,\n",
       " 'human': 1,\n",
       " 'num_ans': 1,\n",
       " 'num_sen': 1}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nq_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e59aa6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NQ 150 Samples InstGPT zero-shot\n",
      "Human accuracy:  0.52\n",
      "BEM accuracy:  0.84\n",
      "NEM_ans accuracy:  0.73\n",
      "NEM_sen accuracy:  0.56\n",
      "\n",
      "BEM accuracy against human label:  0.64\n",
      "NEM_ans accuracy against human label:  0.69\n",
      "NEM_sen accuracy against human label:  0.73\n"
     ]
    }
   ],
   "source": [
    "bem_human=0\n",
    "num_ans_human=0\n",
    "num_sen_human=0\n",
    "human_sum=0\n",
    "bem_sum=0\n",
    "num_ans_sum=0\n",
    "num_sen_sum=0\n",
    "\n",
    "for d in nq_sample:\n",
    "    if d['bem']==d['human']:\n",
    "        bem_human+=1\n",
    "    if d['num_ans']==d['human']:\n",
    "        num_ans_human+=1\n",
    "    if d['num_sen']==d['human']:\n",
    "        num_sen_human+=1\n",
    "    human_sum+=d['human']\n",
    "    bem_sum+=d['bem']\n",
    "    num_ans_sum+=d['num_ans']\n",
    "    num_sen_sum+=d['num_sen']\n",
    "\n",
    "print('NQ 150 Samples InstGPT zero-shot')\n",
    "print('Human accuracy: %5.2f' %(human_sum/150))\n",
    "print('BEM accuracy: %5.2f'%(bem_sum/150))\n",
    "print('NEM_ans accuracy: %5.2f'%(num_ans_sum/150))\n",
    "print('NEM_sen accuracy: %5.2f'%(num_sen_sum/150))\n",
    "print()\n",
    "print('BEM accuracy against human label: %5.2f'%(bem_human/150))\n",
    "print('NEM_ans accuracy against human label: %5.2f'%(num_ans_human/150))\n",
    "print('NEM_sen accuracy against human label: %5.2f'%(num_sen_human/150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "360bdc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_type=['CARDINAL', 'DATE', 'MONEY', 'PERCENT', 'QUANTITY', 'TIME']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c7c2aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "count={d:{\"pos\":0, \"neg\":0, 'sum':0} for d in numeric_type}\n",
    "\n",
    "for d in dev:\n",
    "    count[d['ans_type']]['pos']+=(len(d['ans_pos']))\n",
    "    count[d['ans_type']]['neg']+=len(d['ans_neg'])\n",
    "    count[d['ans_type']]['sum']+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "116f335c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CARDINAL': {'pos': 173, 'neg': 246, 'sum': 60}, 'DATE': {'pos': 289, 'neg': 506, 'sum': 60}, 'MONEY': {'pos': 170, 'neg': 149, 'sum': 59}, 'PERCENT': {'pos': 159, 'neg': 240, 'sum': 52}, 'QUANTITY': {'pos': 263, 'neg': 349, 'sum': 60}, 'TIME': {'pos': 154, 'neg': 154, 'sum': 51}}\n"
     ]
    }
   ],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a108545",
   "metadata": {},
   "outputs": [],
   "source": [
    "count={d:{\"pos\":0, \"neg\":0, 'sum':0} for d in numeric_type}\n",
    "\n",
    "for d in test:\n",
    "    count[d['ans_type']]['pos']+=(len(d['ans_pos']))\n",
    "    count[d['ans_type']]['neg']+=len(d['ans_neg'])\n",
    "    count[d['ans_type']]['sum']+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f9ce088c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CARDINAL': {'pos': 173, 'neg': 223, 'sum': 60}, 'DATE': {'pos': 293, 'neg': 475, 'sum': 60}, 'MONEY': {'pos': 174, 'neg': 175, 'sum': 59}, 'PERCENT': {'pos': 155, 'neg': 236, 'sum': 52}, 'QUANTITY': {'pos': 249, 'neg': 402, 'sum': 60}, 'TIME': {'pos': 155, 'neg': 193, 'sum': 51}}\n"
     ]
    }
   ],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "640c7afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count={d:{\"pos\":0, \"neg\":0, 'sum':0} for d in numeric_type}\n",
    "\n",
    "for d in train:\n",
    "    count[d['ans_type']]['pos']+=(len(d['ans_pos']))\n",
    "    count[d['ans_type']]['neg']+=len(d['ans_neg'])\n",
    "    count[d['ans_type']]['sum']+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "122f6b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CARDINAL': {'pos': 1385, 'neg': 1861, 'sum': 480}, 'DATE': {'pos': 2288, 'neg': 3494, 'sum': 480}, 'MONEY': {'pos': 1416, 'neg': 1450, 'sum': 479}, 'PERCENT': {'pos': 1270, 'neg': 1981, 'sum': 419}, 'QUANTITY': {'pos': 2070, 'neg': 3006, 'sum': 480}, 'TIME': {'pos': 1282, 'neg': 1464, 'sum': 418}}\n"
     ]
    }
   ],
   "source": [
    "print((count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113849b",
   "metadata": {},
   "source": [
    "### Few-shot sq_non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d43d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4784508a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donaldo9603/.conda/envs/numeric/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from eval import bem_score, num_ans_score, num_sen_score,normalize_answer, metric_max_over_ground_truths, soft_exact_match_score, hard_exact_match_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87efba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample150_dir='/home/donaldo9603/workspace/numeric/data/squad/sq_non_150_few_human_final.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc9e2651",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sample150_dir) as f:\n",
    "    sq_sample=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006072f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What were NTL's services rebranded as?\",\n",
       " 'answers': ['Virgin Media', 'Virgin Media', 'Virgin Media'],\n",
       " 'ans_type': 'ORG',\n",
       " 'prediction': ' Virgin Media',\n",
       " 'human': 1,\n",
       " 'bem': 1,\n",
       " 'num': 1,\n",
       " 'em': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9182c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in sq_sample:\n",
    "    bem=bem_score(d['prediction'], d['answers'], d['question'])\n",
    "    num_ans=num_ans_score(d['prediction'], d['answers'], d['question'])\n",
    "    num_sen=num_sen_score(d['prediction'], d['answers'], d['question'])\n",
    "    em=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answers'])\n",
    "    d['bem']=bem\n",
    "    d['num_ans']=num_ans\n",
    "    d['num_sen']=num_sen\n",
    "    d['em']=em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a33c0e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: 0.320\n",
      "BEM: 0.453\n",
      "Human: 0.433\n",
      "EM-recall based on Human label: 0.723\n"
     ]
    }
   ],
   "source": [
    "bem_sum=0\n",
    "em_sum=0\n",
    "human_sum=0\n",
    "em_human_rec=0\n",
    "\n",
    "for d in sq_sample:\n",
    "    bem_sum+=d['bem']\n",
    "    em_sum+=d['em']\n",
    "    human_sum+=d['human']\n",
    "    if d['human']==1 and d['em']==1:\n",
    "        em_human_rec+=1\n",
    "print('EM: %5.3f' %(em_sum/len(sq_sample)))\n",
    "print('BEM: %5.3f' %(bem_sum/len(sq_sample)))\n",
    "\n",
    "print('Human: %5.3f' %(human_sum/len(sq_sample)))\n",
    "print('EM-recall based on Human label: %5.3f' % (em_human_rec/human_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169d1b9",
   "metadata": {},
   "source": [
    "### Few-shot sq-num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "49d08979",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample150_dir='/home/donaldo9603/workspace/numeric/data/squad/sq_num_150_few_human_final.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "59e23ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sample150_dir) as f:\n",
    "    sq_sample=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "53e5e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sample150_dir, 'w') as f:\n",
    "    json.dump(sq_sample,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4ed3fa49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How many will the host committee dedicate to local charities?',\n",
       " 'answers': ['25 percent', '25 percent of all money', '25'],\n",
       " 'ans_type': 'PERCENT',\n",
       " 'prediction': ' $5 million',\n",
       " 'human': 0,\n",
       " 'bem': 0,\n",
       " 'em': 0,\n",
       " 'num_ans': 0,\n",
       " 'num_sen': 1}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a8ab2992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What percent of the global assets in 2000 were owned by just 1% of adults?\n",
      "Answers: ['40%', '40%', '40%', '40']\n",
      "Prediction:  44%\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: How many awards has Doctor Who been nominated for, over the years?\n",
      "Answers: ['over 200', 'over 200', 'over 200']\n",
      "Prediction:  over 1,000 awards\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: What provided for the creation of new orders known as \"provisional elder?\"\n",
      "Answers: ['1996 General Conference', '1996 General Conference the', 'the ordination order of transitional deacon was abolished']\n",
      "Prediction:  The Second Epistle of John\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: For which show did Billie Piper tape an introduction?\n",
      "Answers: ['The Christmas Invasion', 'The Christmas Invasion', 'The Christmas Invasion']\n",
      "Prediction:  Doctor Who Confidential\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: What is Victoria's highest monthly temperature?\n",
      "Answers: ['48.8 °C', '48.8 °C', '48.8 °C (119.8 °F)']\n",
      "Prediction:  46.4 °C (115.5 °F)\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: How much money has been raised by the host committee?\n",
      "Answers: ['$40 million', 'over $40 million', '$40 million']\n",
      "Prediction:  $1.2 billion\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: Approximately how many items are in the V&A's textiles collection?\n",
      "Answers: ['more than 53,000', '53,000', '53,000']\n",
      "Prediction:  70,000\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: How old are the gravestones that reference the plague?\n",
      "Answers: ['dating to 1338–39', '1338–39', '1338–39']\n",
      "Prediction:  17th century\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: What is the time period represented in the museum's textiles collection?\n",
      "Answers: ['from the 1st century AD to the present', '1st century AD to the present,', '1st century AD to the present']\n",
      "Prediction:  6th century CE to the 21st century\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: What 3 things does the Air Force work key on \n",
      "Answers: ['use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks', 'ideas']\n",
      "Prediction:  mission, people, and innovation.\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: What was the population of Kenya in 2014?\n",
      "Answers: ['45 million people', '45 million people', 'approximately 45 million']\n",
      "Prediction:  48.46 million\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: What is the height of the section that turns north? \n",
      "Answers: ['599 m', '599 m to 396 m', '599 m to 396 m', '599 m', '599 m']\n",
      "Prediction:  8,000 feet (2,400 m)\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: what was Fresno's population in 2010?\n",
      "Answers: ['494,665', '494,665', '494,665']\n",
      "Prediction:  495,000\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: bassett focuses on what to illustrate his idea?\n",
      "Answers: ['nineteenth-century maps', 'nineteenth-century maps', 'the role of nineteenth-century maps', 'the role of nineteenth-century maps', 'the role of nineteenth-century maps during the \"scramble for Africa\"']\n",
      "Prediction:  the human brain\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: How many tons of dust remains in the air?\n",
      "Answers: ['132 million tons', '132 million tons', '132 million']\n",
      "Prediction:  5-50 tons\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: How may yards did Peyton Manning throw?\n",
      "Answers: ['2,249', '2,249', '2,249']\n",
      "Prediction:  59,487 yards\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: How many seats does Victoria have in the Australian House of Representatives?\n",
      "Answers: ['37', '37', '37']\n",
      "Prediction:  37\n",
      "Human Label: 1\n",
      "Model label: 0\n",
      "\n",
      "Question: When was the paper published that the \"Millennial Northern Hemisphere temperature reconstruction\" graph was based on?\n",
      "Answers: ['1999', '1999', '1999']\n",
      "Prediction:  1999\n",
      "Human Label: 1\n",
      "Model label: 0\n",
      "\n",
      "Question: How much did Tesla spend on the injured pigeon?\n",
      "Answers: ['over $2,000', 'over $2,000,', 'over $2,000']\n",
      "Prediction:  $2\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: What is the law of thermodynamics associated with closed system heat exchange?\n",
      "Answers: ['Second', 'Second law of thermodynamics', 'Second law', 'Second']\n",
      "Prediction:  The Zeroth Law of Thermodynamics\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: How much of the population is Hindu?\n",
      "Answers: ['around 300,000', '300,000', '300,000']\n",
      "Prediction:  80.5%\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: What year did the the Saints hit a 13-0 record?\n",
      "Answers: ['2009', '2009', '2009']\n",
      "Prediction:  2009\n",
      "Human Label: 1\n",
      "Model label: 0\n",
      "\n",
      "Question: How many kilometers is Warsaw from the Carpathian Mountains?\n",
      "Answers: ['about 300', '300', '300']\n",
      "Prediction:  approximately 250 km\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: How long were the fighters of the Warsaw Ghetto Uprising able to hold out?\n",
      "Answers: ['almost a month', 'almost a month', 'almost a month']\n",
      "Prediction:  27 days\n",
      "Human Label: 1\n",
      "Model label: 0\n",
      "\n",
      "Question: How many were in Langlades expedition?\n",
      "Answers: ['300 men, including French-Canadians and warriors of the Ottawa', '300', '300 men', '300', '300 men']\n",
      "Prediction:  45\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: At what temperature will oxygen condense?\n",
      "Answers: ['90.20 K', '90.20 K', '90.20 K (−182.95 °C, −297.31 °F)', '90.20 K', '90.20 K (−182.95 °C, −297.31 °F)']\n",
      "Prediction:  -218.8°C (-361.8°F)\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: In what century did the history of the steam engine begin?\n",
      "Answers: ['first', 'first century AD', 'first century AD', 'first century AD']\n",
      "Prediction:  18th century\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: What logo is used for all merchandise that features past Doctors?\n",
      "Answers: ['the logo used for the Third and Eighth Doctors', 'the Third and Eighth Doctors', 'the logo used for the Third and Eighth Doctors']\n",
      "Prediction:  The Whoosign\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: When did the four rightly guided Caliphs die?\n",
      "Answers: ['7th century', 'the 7th century', '7th century']\n",
      "Prediction:  661 AD\n",
      "Human Label: 1\n",
      "Model label: 0\n",
      "\n",
      "Question: How large can ctenophora grow?\n",
      "Answers: ['1.5 m (4 ft 11 in)', 'a few millimeters to 1.5 m']\n",
      "Prediction:  up to 20 cm in diameter\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: What is the number of plant species in economics and social interest?\n",
      "Answers: ['438,000', '438,000', '438,000', '438,000 species']\n",
      "Prediction:  41,000\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: What is the average construction salary in the UK?\n",
      "Answers: ['£26,719', '£26,719', '£26,719']\n",
      "Prediction:  £35,000 per year\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: How quickly is the sea level rising?\n",
      "Answers: ['1–3 cm (0.39–1.18 in) per century', '1–3 cm (0.39–1.18 in) per century', '1–3 cm (0.39–1.18 in) per century']\n",
      "Prediction:  about 3.2 millimeters per year\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: Pitt's plan called for what attacks?\n",
      "Answers: ['three major offensive actions involving large numbers of regular troops', 'three major offensive actions', 'three major offensive actions', 'three major offensive actions', 'three major offensive actions']\n",
      "Prediction:  amphibious landings\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: When were the public housing developments built in the neighborhood?\n",
      "Answers: ['between the 1960s and 1990s', 'between the 1960s and 1990s', 'between the 1960s and 1990s']\n",
      "Prediction:  1950s and 1960s\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: What was the resolution of the cameras used in the EyeVision 360 system?\n",
      "Answers: ['5K', '5K', '5K']\n",
      "Prediction:  1.2 megapixel resolution\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: What did ABC do that was special in 2003?\n",
      "Answers: ['weekly screenings of all available classic episodes', 'screenings of all available classic episodes', 'repeated episodes']\n",
      "Prediction:  aired its first Super Bowl Halftime Show\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: When did the formation of the Holocene Rhine-Meuse delta begin?\n",
      "Answers: ['8,000 years ago', '~8,000 years ago', '~8,000 years ago']\n",
      "Prediction:  8,500 years ago\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: How much oxygen is found is a liter of fresh water under normal conditions?\n",
      "Answers: ['6.04 milliliters', '6.04 milliliters', '6.04 milliliters', '6.04 milliliters', '6.04 milliliters']\n",
      "Prediction:  8–14 mg/L\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: What is the duration of Harvard Academic year?\n",
      "Answers: ['beginning in early September and ending in mid-May', 'beginning in early September and ending in mid-May', 'beginning in early September and ending in mid-May']\n",
      "Prediction:  8 months (September to May)\n",
      "Human Label: 1\n",
      "Model label: 0\n",
      "\n",
      "Question: When did the origins of magnetic and electric fields occur?\n",
      "Answers: ['1864', '1864', '1864', '1864']\n",
      "Prediction:  4.6 billion years ago\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: How many buildings were razed by the Jacksonville fire?\n",
      "Answers: ['over 2,000', '2,000 buildings', 'over 2,']\n",
      "Prediction:  over 2,500\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: What omen was Genghis Khan reported to have seen assuring his coming victory against the Tanguts?\n",
      "Answers: ['a line of five stars arranged in the sky', 'a line of five stars', 'a line of five stars arranged in the sky']\n",
      "Prediction:  a white falcon\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: What was result of French attack of trading centre?\n",
      "Answers: ['capturing three traders and killing 14 people of the Miami nation, including Old Briton', 'capturing three traders and killing 14 people of the Miami nation', 'capturing three traders and killing 14 people of the Miami nation, including Old Briton', 'capturing three traders and killing 14 people', 'capturing three traders and killing 14 people of the Miami nation']\n",
      "Prediction:  The French were repelled by a small but determined Dutch and English garrison.\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n",
      "Question: At this time where was Luther's focus centered?\n",
      "Answers: ['Daniel 8:9–12, 23–25', 'prophecy', 'prophecy of the Little Horn']\n",
      "Prediction:  the Bible and the Christian life\n",
      "Human Label: 0\n",
      "Model label: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in sq_sample:\n",
    "    if d['num_ans']!= d['human']:\n",
    "        print('Question: {}'.format(d['question']))\n",
    "        print('Answers: {}'.format(d['answers']))\n",
    "        print('Prediction: {}'.format(d['prediction']))\n",
    "        print('Human Label: {}'.format(d['human']))\n",
    "        print('Model label: {}'.format(d['num_ans']))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "074ff68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in sq_sample:\n",
    "    bem=bem_score(d['prediction'], d['answers'], d['question'])\n",
    "    num_ans=num_ans_score(d['prediction'], d['answers'], d['question'])\n",
    "    num_sen=num_sen_score(d['prediction'], d['answers'], d['question'])\n",
    "    em=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answers'])\n",
    "    d['bem']=bem\n",
    "    d['num_ans']=num_ans\n",
    "    d['num_sen']=num_sen\n",
    "    d['em']=em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c1a5f4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: 0.227\n",
      "BEM: 0.540\n",
      "Human: 0.313\n",
      "EM-recall based on Human label: 0.702\n"
     ]
    }
   ],
   "source": [
    "bem_sum=0\n",
    "em_sum=0\n",
    "human_sum=0\n",
    "em_human_rec=0\n",
    "\n",
    "for d in sq_sample:\n",
    "    bem_sum+=d['bem']\n",
    "    em_sum+=d['em']\n",
    "    human_sum+=d['human']\n",
    "    if d['human']==1 and d['em']==1:\n",
    "        em_human_rec+=1\n",
    "print('EM: %5.3f' %(em_sum/len(sq_sample)))\n",
    "print('BEM: %5.3f' %(bem_sum/len(sq_sample)))\n",
    "\n",
    "print('Human: %5.3f' %(human_sum/len(sq_sample)))\n",
    "print('EM-recall based on Human label: %5.3f' % (em_human_rec/human_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b8a49f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQUAD 150 Samples InstGPT few-shot\n",
      "Human accuracy:  0.31\n",
      "BEM accuracy:  0.54\n",
      "NEM_ans accuracy:  0.53\n",
      "NEM_sen accuracy:  0.39\n",
      "\n",
      "BEM accuracy against human label:  0.73\n",
      "NEM_ans accuracy against human label:  0.70\n",
      "NEM_sen accuracy against human label:  0.53\n"
     ]
    }
   ],
   "source": [
    "bem_human=0\n",
    "num_ans_human=0\n",
    "num_sen_human=0\n",
    "human_sum=0\n",
    "bem_sum=0\n",
    "num_ans_sum=0\n",
    "num_sen_sum=0\n",
    "\n",
    "for d in sq_sample:\n",
    "    if d['bem']==d['human']:\n",
    "        bem_human+=1\n",
    "    if d['num_ans']==d['human']:\n",
    "        num_ans_human+=1\n",
    "    if d['num_sen']==d['human']:\n",
    "        num_sen_human+=1\n",
    "    human_sum+=d['human']\n",
    "    bem_sum+=d['bem']\n",
    "    num_ans_sum+=d['num_ans']\n",
    "    num_sen_sum+=d['num_sen']\n",
    "\n",
    "print('SQUAD 150 Samples InstGPT few-shot')\n",
    "print('Human accuracy: %5.2f' %(human_sum/150))\n",
    "print('BEM accuracy: %5.2f'%(bem_sum/150))\n",
    "print('NEM_ans accuracy: %5.2f'%(num_ans_sum/150))\n",
    "print('NEM_sen accuracy: %5.2f'%(num_sen_sum/150))\n",
    "print()\n",
    "print('BEM accuracy against human label: %5.2f'%(bem_human/150))\n",
    "print('NEM_ans accuracy against human label: %5.2f'%(num_ans_human/150))\n",
    "print('NEM_sen accuracy against human label: %5.2f'%(num_sen_human/150))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f061b",
   "metadata": {},
   "source": [
    "### Few-shot nq-non"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a846709",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample150_dir='/home/donaldo9603/workspace/numeric/data/nq/nq_non_150_few_human_final.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9ad5cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sample150_dir) as f:\n",
    "    nq_sample=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "69bf3940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1982,\n",
       " 'question': 'who is the head of the department of homeland security 2017',\n",
       " 'answer': ['Kirstjen Nielsen'],\n",
       " 'ans_type': 'PERSON',\n",
       " 'prediction': ' Kirstjen Nielsen',\n",
       " 'human': 1,\n",
       " 'bem': 1,\n",
       " 'num': 1,\n",
       " 'em': 1}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nq_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ff0ab9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: 0.533\n",
      "BEM: 0.593\n",
      "Human: 0.593\n",
      "EM-recall based on Human label: 0.876\n"
     ]
    }
   ],
   "source": [
    "bem_sum=0\n",
    "em_sum=0\n",
    "human_sum=0\n",
    "\n",
    "em_human_rec=0\n",
    "\n",
    "for d in nq_sample:\n",
    "    bem_sum+=d['bem']\n",
    "    em_sum+=d['em']\n",
    "    human_sum+=d['human']\n",
    "    \n",
    "    if d['human']==1 and d['em'] ==1:\n",
    "        em_human_rec+=1\n",
    "\n",
    "print('EM: %5.3f' %(em_sum/len(nq_sample)))\n",
    "print('BEM: %5.3f' %(bem_sum/len(nq_sample)))\n",
    "\n",
    "\n",
    "print('Human: %5.3f' %(human_sum/len(nq_sample)))\n",
    "print('EM-recall based on Human label: %5.3f' % (em_human_rec/human_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd170d",
   "metadata": {},
   "source": [
    "### Few-shot nq_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633c5067",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample150_dir='/home/donaldo9603/workspace/numeric/data/nq/nq_num_150_few_human_final.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29eb61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sample150_dir) as f:\n",
    "    nq_sample=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb762c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c3a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in nq_sample:\n",
    "    bem=bem_score(d['prediction'], d['answer'], d['question'])\n",
    "    num_ans=num_ans_score(d['prediction'], d['answer'], d['question'])\n",
    "    num_sen=num_sen_score(d['prediction'], d['answer'], d['question'])\n",
    "    em=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answer'])\n",
    "    d['bem']=bem\n",
    "    d['num_ans']=num_ans\n",
    "    d['num_sen']=num_sen\n",
    "    d['em']=em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08be3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bem_sum=0\n",
    "em_sum=0\n",
    "human_sum=0\n",
    "\n",
    "em_human_rec=0\n",
    "\n",
    "for d in nq_sample:\n",
    "    bem_sum+=d['bem']\n",
    "    em_sum+=d['em']\n",
    "    human_sum+=d['human']\n",
    "    \n",
    "    if d['human']==1 and d['em'] ==1:\n",
    "        em_human_rec+=1\n",
    "\n",
    "print('EM: %5.3f' %(em_sum/len(nq_sample)))\n",
    "print('BEM: %5.3f' %(bem_sum/len(nq_sample)))\n",
    "\n",
    "\n",
    "print('Human: %5.3f' %(human_sum/len(nq_sample)))\n",
    "print('EM-recall based on Human label: %5.3f' % (em_human_rec/human_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0a7dad46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NQ 150 Samples InstGPT few-shot\n",
      "Human accuracy:  0.55\n",
      "BEM accuracy:  0.70\n",
      "NEM_ans accuracy:  0.48\n",
      "NEM_sen accuracy:  0.27\n",
      "\n",
      "BEM accuracy against human label:  0.79\n",
      "NEM_ans accuracy against human label:  0.89\n",
      "NEM_sen accuracy against human label:  0.63\n"
     ]
    }
   ],
   "source": [
    "bem_human=0\n",
    "num_ans_human=0\n",
    "num_sen_human=0\n",
    "human_sum=0\n",
    "bem_sum=0\n",
    "num_ans_sum=0\n",
    "num_sen_sum=0\n",
    "\n",
    "for d in nq_sample:\n",
    "    if d['bem']==d['human']:\n",
    "        bem_human+=1\n",
    "    if d['num_ans']==d['human']:\n",
    "        num_ans_human+=1\n",
    "    if d['num_sen']==d['human']:\n",
    "        num_sen_human+=1\n",
    "    human_sum+=d['human']\n",
    "    bem_sum+=d['bem']\n",
    "    num_ans_sum+=d['num_ans']\n",
    "    num_sen_sum+=d['num_sen']\n",
    "\n",
    "print('NQ 150 Samples InstGPT few-shot')\n",
    "print('Human accuracy: %5.2f' %(human_sum/150))\n",
    "print('BEM accuracy: %5.2f'%(bem_sum/150))\n",
    "print('NEM_ans accuracy: %5.2f'%(num_ans_sum/150))\n",
    "print('NEM_sen accuracy: %5.2f'%(num_sen_sum/150))\n",
    "print()\n",
    "print('BEM accuracy against human label: %5.2f'%(bem_human/150))\n",
    "print('NEM_ans accuracy against human label: %5.2f'%(num_ans_human/150))\n",
    "print('NEM_sen accuracy against human label: %5.2f'%(num_sen_human/150))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518ca000",
   "metadata": {},
   "source": [
    "### Non numeric human vs EM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "14f89f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_non_dir='/home/donaldo9603/workspace/numeric/data/nq/nq_non_150_few_human_final.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5f65fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(nq_non_dir) as f:\n",
    "    nq_non=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8eb50766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1982,\n",
       " 'question': 'who is the head of the department of homeland security 2017',\n",
       " 'answer': ['Kirstjen Nielsen'],\n",
       " 'ans_type': 'PERSON',\n",
       " 'prediction': ' Kirstjen Nielsen',\n",
       " 'human': 1,\n",
       " 'bem': 1,\n",
       " 'num': 1,\n",
       " 'em': 1}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nq_non[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d26dae52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5933333333333334\n",
      "0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "human=0\n",
    "em=0\n",
    "for d in nq_non:\n",
    "    human+=d['human']\n",
    "    em+=d['em']\n",
    "print(human/150)\n",
    "print(em/150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "975da36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1e9c6d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_dev=[]\n",
    "with jsonlines.open('/home/donaldo9603/workspace/numeric/R2-D2/squad_eval/SQUAD_dev.jsonl') as f:\n",
    "    for line in f:\n",
    "        sq_dev.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1c08b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_pred=[]\n",
    "with jsonlines.open('/home/donaldo9603/workspace/numeric/R2-D2/squad_output/predictions.jsonl') as f:\n",
    "    for line in f:\n",
    "        sq_pred.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5eebcacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10570\n",
      "10539\n"
     ]
    }
   ],
   "source": [
    "print(len(sq_dev))\n",
    "print(len(sq_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ed069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda:.conda-numeric]",
   "language": "python",
   "name": "conda-env-.conda-numeric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
