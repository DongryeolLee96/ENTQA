{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb694667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0edebe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donaldo9603/.conda/envs/numeric/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from eval import num_integ_score, bem_score, num_ans_score, num_sen_score,normalize_answer, metric_max_over_ground_truths, soft_exact_match_score, hard_exact_match_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d12fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_num_zero_dir='/home/donaldo9603/workspace/numeric/data/nq/nq_num_150_zero_human_final.json'\n",
    "nq_non_zero_dir='/home/donaldo9603/workspace/numeric/data/nq/nq_non_150_zero_human_final.json'\n",
    "nq_num_few_dir='/home/donaldo9603/workspace/numeric/data/nq/nq_num_150_few_human_final.json'\n",
    "nq_non_few_dir='/home/donaldo9603/workspace/numeric/data/nq/nq_non_150_few_human_final.json'\n",
    "\n",
    "nq_num_emdr_dir='/home/donaldo9603/workspace/numeric/data/nq/nq_num_150_emdr_human_final.json'\n",
    "nq_non_emdr_dir='/home/donaldo9603/workspace/numeric/data/nq/nq_non_150_emdr_human_final.json'\n",
    "nq_num_r2d2_dir='/home/donaldo9603/workspace/numeric/data/nq/nq_num_150_r2d2_human_final.json'\n",
    "nq_non_r2d2_dir='/home/donaldo9603/workspace/numeric/data/nq/nq_non_150_r2d2_human_final.json'\n",
    "\n",
    "sq_num_zero_dir='/home/donaldo9603/workspace/numeric/data/squad/sq_num_150_zero_human_final.json'\n",
    "sq_non_zero_dir='/home/donaldo9603/workspace/numeric/data/squad/sq_non_150_zero_human_final.json'\n",
    "sq_num_few_dir='/home/donaldo9603/workspace/numeric/data/squad/sq_num_150_few_human_final.json'\n",
    "sq_non_few_dir='/home/donaldo9603/workspace/numeric/data/squad/sq_non_150_few_human_final.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b5fb011",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(nq_num_zero_dir) as f:\n",
    "    nq_num_zero=json.load(f)\n",
    "with open(nq_non_zero_dir) as f:\n",
    "    nq_non_zero=json.load(f)\n",
    "with open(nq_num_few_dir) as f:\n",
    "    nq_num_few=json.load(f)\n",
    "with open(nq_non_few_dir) as f:\n",
    "    nq_non_few=json.load(f)\n",
    "    \n",
    "with open(nq_num_emdr_dir) as f:\n",
    "    nq_num_emdr=json.load(f)\n",
    "with open(nq_non_emdr_dir) as f:\n",
    "    nq_non_emdr=json.load(f)\n",
    "with open(nq_num_r2d2_dir) as f:\n",
    "    nq_num_r2d2=json.load(f)\n",
    "with open(nq_non_r2d2_dir) as f:\n",
    "    nq_non_r2d2=json.load(f)\n",
    "    \n",
    "    \n",
    "with open(sq_num_zero_dir) as f:\n",
    "    sq_num_zero=json.load(f)\n",
    "with open(sq_non_zero_dir) as f:\n",
    "    sq_non_zero=json.load(f)\n",
    "with open(sq_num_few_dir) as f:\n",
    "    sq_num_few=json.load(f)\n",
    "with open(sq_non_few_dir) as f:\n",
    "    sq_non_few=json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64fbb306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what was the population of the roman empire at its height',\n",
       " 'answer': ['50 to 90\\xa0million inhabitants',\n",
       "  '70\\xa0million',\n",
       "  '55â€“60 million',\n",
       "  'an estimated 70\\xa0million people'],\n",
       " 'ans_type': 'CARDINAL',\n",
       " 'prediction': '\\n\\nThe population of the Roman Empire at its height is estimated to have been between 50 and 80 million people.',\n",
       " 'human': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nq_num_zero[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69d7edb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 3064,\n",
       " 'question': 'what was the population of the roman empire at its height',\n",
       " 'answer': ['50 to 90 million inhabitants',\n",
       "  '70 million',\n",
       "  '55-60 million',\n",
       "  'an estimated 70 million people'],\n",
       " 'ans_type': 'CARDINAL',\n",
       " 'prediction': ' ~55 million',\n",
       " 'human': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nq_num_few[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef63a1e",
   "metadata": {},
   "source": [
    "### Numeric vs Non-numeric - Human - Soft EM - Hard EM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87817ed8",
   "metadata": {},
   "source": [
    "### NQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c30ddda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nq_num_zeroshot\n",
      "Human evaluation:  0.53\n",
      "Soft EM evaluation:  0.37\n",
      "Hard EM evaluation:  0.01\n",
      "F1 score:  0.13\n"
     ]
    }
   ],
   "source": [
    "human_sum=0\n",
    "soft_em_sum=0\n",
    "hard_em_sum=0\n",
    "f1_sum=0\n",
    "for d in nq_num_zero:\n",
    "    human_sum+=d['human']\n",
    "    soft_em_sum+=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answer'])\n",
    "    hard_em_sum+=metric_max_over_ground_truths(hard_exact_match_score, d['prediction'], d['answer'])\n",
    "    f1_sum+=metric_max_over_ground_truths(f1_score, d['prediction'], d['answer'])\n",
    "print('nq_num_zeroshot')\n",
    "print('Human evaluation: %5.2f' % (human_sum/len(nq_num_zero)))\n",
    "print('Soft EM evaluation: %5.2f' % (soft_em_sum/len(nq_num_zero)))\n",
    "print('Hard EM evaluation: %5.2f' % (hard_em_sum/len(nq_num_zero)))\n",
    "print('F1 score: %5.2f' % (f1_sum/len(nq_num_zero)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "700890f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nq_non_zeroshot\n",
      "Human evaluation:  0.57\n",
      "Soft EM evaluation:  0.51\n",
      "Hard EM evaluation:  0.03\n",
      "F1 score:  0.17\n"
     ]
    }
   ],
   "source": [
    "human_sum=0\n",
    "soft_em_sum=0\n",
    "hard_em_sum=0\n",
    "f1_sum=0\n",
    "\n",
    "for d in nq_non_zero:\n",
    "    human_sum+=d['human']\n",
    "    soft_em_sum+=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answer'])\n",
    "    hard_em_sum+=metric_max_over_ground_truths(hard_exact_match_score, d['prediction'], d['answer'])\n",
    "    f1_sum+=metric_max_over_ground_truths(f1_score, d['prediction'], d['answer'])\n",
    "\n",
    "print('nq_non_zeroshot')\n",
    "print('Human evaluation: %5.2f' % (human_sum/len(nq_num_zero)))\n",
    "print('Soft EM evaluation: %5.2f' % (soft_em_sum/len(nq_num_zero)))\n",
    "print('Hard EM evaluation: %5.2f' % (hard_em_sum/len(nq_num_zero)))\n",
    "print('F1 score: %5.2f' % (f1_sum/len(nq_num_zero)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cd944c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nq_num_fewshot\n",
      "Human evaluation:  0.56\n",
      "Soft EM evaluation:  0.42\n",
      "Hard EM evaluation:  0.33\n",
      "F1 score:  0.50\n"
     ]
    }
   ],
   "source": [
    "human_sum=0\n",
    "soft_em_sum=0\n",
    "hard_em_sum=0\n",
    "f1_sum=0\n",
    "\n",
    "for d in nq_num_few:\n",
    "    human_sum+=d['human']\n",
    "    soft_em_sum+=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answer'])\n",
    "    hard_em_sum+=metric_max_over_ground_truths(hard_exact_match_score, d['prediction'], d['answer'])\n",
    "    f1_sum+=metric_max_over_ground_truths(f1_score, d['prediction'], d['answer'])\n",
    "\n",
    "print('nq_num_fewshot')\n",
    "print('Human evaluation: %5.2f' % (human_sum/len(nq_num_zero)))\n",
    "print('Soft EM evaluation: %5.2f' % (soft_em_sum/len(nq_num_zero)))\n",
    "print('Hard EM evaluation: %5.2f' % (hard_em_sum/len(nq_num_zero)))\n",
    "print('F1 score: %5.2f' % (f1_sum/len(nq_num_zero)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c259831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nq_non_fewshot\n",
      "Human evaluation:  0.59\n",
      "Soft EM evaluation:  0.53\n",
      "Hard EM evaluation:  0.46\n",
      "F1 score:  0.55\n"
     ]
    }
   ],
   "source": [
    "human_sum=0\n",
    "soft_em_sum=0\n",
    "hard_em_sum=0\n",
    "f1_sum=0\n",
    "\n",
    "for d in nq_non_few:\n",
    "    human_sum+=d['human']\n",
    "    soft_em_sum+=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answer'])\n",
    "    hard_em_sum+=metric_max_over_ground_truths(hard_exact_match_score, d['prediction'], d['answer'])\n",
    "    f1_sum+=metric_max_over_ground_truths(f1_score, d['prediction'], d['answer'])\n",
    "\n",
    "print('nq_non_fewshot')\n",
    "print('Human evaluation: %5.2f' % (human_sum/len(nq_num_zero)))\n",
    "print('Soft EM evaluation: %5.2f' % (soft_em_sum/len(nq_num_zero)))\n",
    "print('Hard EM evaluation: %5.2f' % (hard_em_sum/len(nq_num_zero)))\n",
    "print('F1 score: %5.2f' % (f1_sum/len(nq_num_zero)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e69049",
   "metadata": {},
   "source": [
    "### NQ R2D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1809abab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nq_num_r2d2\n",
      "Human evaluation:  0.69\n",
      "Soft EM evaluation:  0.61\n",
      "Hard EM evaluation:  0.57\n",
      "F1 score:  0.66\n"
     ]
    }
   ],
   "source": [
    "human_sum=0\n",
    "soft_em_sum=0\n",
    "hard_em_sum=0\n",
    "f1_sum=0\n",
    "\n",
    "for d in nq_num_r2d2:\n",
    "    human_sum+=d['human']\n",
    "    soft_em_sum+=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answer'])\n",
    "    hard_em_sum+=metric_max_over_ground_truths(hard_exact_match_score, d['prediction'], d['answer'])\n",
    "    f1_sum+=metric_max_over_ground_truths(f1_score, d['prediction'], d['answer'])\n",
    "\n",
    "print('nq_num_r2d2')\n",
    "print('Human evaluation: %5.2f' % (human_sum/len(nq_num_zero)))\n",
    "print('Soft EM evaluation: %5.2f' % (soft_em_sum/len(nq_num_zero)))\n",
    "print('Hard EM evaluation: %5.2f' % (hard_em_sum/len(nq_num_zero)))\n",
    "print('F1 score: %5.2f' % (f1_sum/len(nq_num_zero)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81fc7fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nq_non_r2d2\n",
      "Human evaluation:  0.71\n",
      "Soft EM evaluation:  0.64\n",
      "Hard EM evaluation:  0.63\n",
      "F1 score:  0.69\n"
     ]
    }
   ],
   "source": [
    "human_sum=0\n",
    "soft_em_sum=0\n",
    "hard_em_sum=0\n",
    "f1_sum=0\n",
    "\n",
    "for d in nq_non_r2d2:\n",
    "    human_sum+=d['human']\n",
    "    soft_em_sum+=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answer'])\n",
    "    hard_em_sum+=metric_max_over_ground_truths(hard_exact_match_score, d['prediction'], d['answer'])\n",
    "    f1_sum+=metric_max_over_ground_truths(f1_score, d['prediction'], d['answer'])\n",
    "\n",
    "print('nq_non_r2d2')\n",
    "print('Human evaluation: %5.2f' % (human_sum/len(nq_num_zero)))\n",
    "print('Soft EM evaluation: %5.2f' % (soft_em_sum/len(nq_num_zero)))\n",
    "print('Hard EM evaluation: %5.2f' % (hard_em_sum/len(nq_num_zero)))\n",
    "print('F1 score: %5.2f' % (f1_sum/len(nq_num_zero)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8b5cb",
   "metadata": {},
   "source": [
    "### NQ EMDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff0838b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nq_num_emdr\n",
      "Human evaluation:  0.67\n",
      "Soft EM evaluation:  0.64\n",
      "Hard EM evaluation:  0.57\n",
      "F1 score:  0.67\n"
     ]
    }
   ],
   "source": [
    "human_sum=0\n",
    "soft_em_sum=0\n",
    "hard_em_sum=0\n",
    "f1_sum=0\n",
    "\n",
    "for d in nq_num_emdr:\n",
    "    human_sum+=d['human']\n",
    "    soft_em_sum+=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answer'])\n",
    "    hard_em_sum+=metric_max_over_ground_truths(hard_exact_match_score, d['prediction'], d['answer'])\n",
    "    f1_sum+=metric_max_over_ground_truths(f1_score, d['prediction'], d['answer'])\n",
    "\n",
    "print('nq_num_emdr')\n",
    "print('Human evaluation: %5.2f' % (human_sum/len(nq_num_zero)))\n",
    "print('Soft EM evaluation: %5.2f' % (soft_em_sum/len(nq_num_zero)))\n",
    "print('Hard EM evaluation: %5.2f' % (hard_em_sum/len(nq_num_zero)))\n",
    "print('F1 score: %5.2f' % (f1_sum/len(nq_num_zero)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e57da9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nq_non_emdr\n",
      "Human evaluation:  0.72\n",
      "Soft EM evaluation:  0.68\n",
      "Hard EM evaluation:  0.67\n",
      "F1 score:  0.71\n"
     ]
    }
   ],
   "source": [
    "human_sum=0\n",
    "soft_em_sum=0\n",
    "hard_em_sum=0\n",
    "f1_sum=0\n",
    "\n",
    "for d in nq_non_emdr:\n",
    "    human_sum+=d['human']\n",
    "    soft_em_sum+=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answer'])\n",
    "    hard_em_sum+=metric_max_over_ground_truths(hard_exact_match_score, d['prediction'], d['answer'])\n",
    "    f1_sum+=metric_max_over_ground_truths(f1_score, d['prediction'], d['answer'])\n",
    "\n",
    "print('nq_non_emdr')\n",
    "print('Human evaluation: %5.2f' % (human_sum/len(nq_num_zero)))\n",
    "print('Soft EM evaluation: %5.2f' % (soft_em_sum/len(nq_num_zero)))\n",
    "print('Hard EM evaluation: %5.2f' % (hard_em_sum/len(nq_num_zero)))\n",
    "print('F1 score: %5.2f' % (f1_sum/len(nq_num_zero)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f2c403",
   "metadata": {},
   "source": [
    "### SQUAD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a37f5601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human evaluation:  0.26\n",
      "Soft EM evaluation:  0.20\n",
      "Hard EM evaluation:  0.00\n",
      "F1 score:  0.11\n"
     ]
    }
   ],
   "source": [
    "human_sum=0\n",
    "soft_em_sum=0\n",
    "hard_em_sum=0\n",
    "f1_sum=0\n",
    "\n",
    "for d in sq_num_zero:\n",
    "    human_sum+=d['human']\n",
    "    soft_em_sum+=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answers'])\n",
    "    hard_em_sum+=metric_max_over_ground_truths(hard_exact_match_score, d['prediction'], d['answers'])\n",
    "    f1_sum+=metric_max_over_ground_truths(f1_score, d['prediction'], d['answers'])\n",
    "\n",
    "print('Human evaluation: %5.2f' % (human_sum/len(nq_num_zero)))\n",
    "print('Soft EM evaluation: %5.2f' % (soft_em_sum/len(nq_num_zero)))\n",
    "print('Hard EM evaluation: %5.2f' % (hard_em_sum/len(nq_num_zero)))\n",
    "print('F1 score: %5.2f' % (f1_sum/len(nq_num_zero)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3476034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many will the host committee dedicate to local charities?\n",
      "['25 percent', '25 percent of all money', '25']\n",
      "\n",
      "\n",
      "The exact amount the host committee will dedicate to local charities cannot be determined without additional information, such as the budget for the event or the total amount of funds raised.\n",
      "\n",
      "In what year did a tropical storm cause a four day loss of power to Jacksonville?\n",
      "['2008', 'Fay', '2008']\n",
      "\n",
      "\n",
      "In 2017, Tropical Storm Irma caused a four-day power outage in Jacksonville, Florida.\n",
      "\n",
      "What is the weight of a bushel of coal in pounds?\n",
      "['94', '94 pounds', '94 pounds']\n",
      "\n",
      "\n",
      "A bushel of coal typically weighs between 60 and 80 pounds.\n",
      "\n",
      "How many divisions was ABC radio restructured into in 2005?\n",
      "['six divisions', 'six', 'six']\n",
      "\n",
      "\n",
      "ABC radio was restructured into four divisions in 2005. These divisions are ABC Local Radio, ABC Radio National, ABC Classic FM, and Triple J.\n",
      "\n",
      "How many awards has Doctor Who been nominated for, over the years?\n",
      "['over 200', 'over 200', 'over 200']\n",
      "\n",
      "\n",
      "Doctor Who has been nominated for over 500 awards since its first broadcast in 1963.\n",
      "\n",
      "How tall was Tesla?\n",
      "['6 feet 2 inches', '6 feet 2 inches', '6 feet 2 inches (1.88 m)']\n",
      "\n",
      "\n",
      "Nikola Tesla was approximately 5 feet 7 inches (1.7 meters) tall.\n",
      "\n",
      "How many Saturn V rockets were produced by NASA during the Apollo project?\n",
      "['15', '15', '15', '15 Saturn V rockets', '15']\n",
      "\n",
      "\n",
      "NASA produced a total of 13 Saturn V rockets during the Apollo program.\n",
      "\n",
      "What year was North America's first printing press started?\n",
      "['1638', '1638', '1638']\n",
      "\n",
      "\n",
      "The first printing press in North America was started in 1639 in Cambridge, Massachusetts.\n",
      "\n",
      "How large are granal thylakoids?\n",
      "['about 300â€“600 nanometers in diameter', '300â€“600 nanometers', '300â€“600 nanometers in diameter']\n",
      "\n",
      "\n",
      "Granal thylakoids range in size from 0.1 to 0.5 microns in diameter.\n",
      "\n",
      "What provided for the creation of new orders known as \"provisional elder?\"\n",
      "['1996 General Conference', '1996 General Conference the', 'the ordination order of transitional deacon was abolished']\n",
      "\n",
      "\n",
      "Provisional elders were created by the United Methodist Church in response to the growing need for clergy in various parts of the world. These provisional elders are able to provide spiritual leadership and pastoral care in areas where there is a shortage of ordained clergy. They are not ordained, but are appointed to serve particular churches or ministries on a temporary basis.\n",
      "\n",
      "When was the soundtrack for series 5 released?\n",
      "['8 November 2010', '8 November 2010', '8 November 2010']\n",
      "\n",
      "\n",
      "The soundtrack for series 5 of the show was released on July 10, 2020.\n",
      "\n",
      "How many total yards did Denver gain?\n",
      "['194', '194', '194']\n",
      "\n",
      "\n",
      "Denver gained 496 total yards in the game.\n",
      "\n",
      "How many tackles did Von Miller get during the game?\n",
      "['5', 'five', 'five']\n",
      "\n",
      "\n",
      "It is not possible to answer this question without knowing which game you are referring to.\n",
      "\n",
      "For which show did Billie Piper tape an introduction?\n",
      "['The Christmas Invasion', 'The Christmas Invasion', 'The Christmas Invasion']\n",
      "\n",
      "\n",
      "Billie Piper recorded an introduction for the show Doctor Who, which she starred in from 2005 to 2006.\n",
      "\n",
      "In what year did Tesla demonstrate his alternating current system?\n",
      "['1888', '1888', '1888']\n",
      "\n",
      "\n",
      "Tesla demonstrated his alternating current system in 1887.\n",
      "\n",
      "In what year did the scavenger hunt begin?\n",
      "['1987', '1987', '1987']\n",
      "\n",
      "\n",
      "The scavenger hunt began in 2012.\n",
      "\n",
      "In what year did Tesla's family move to Gospic?\n",
      "['1862', '1862', '1862']\n",
      "\n",
      "\n",
      "Tesla's family moved to Gospic in 1879.\n",
      "\n",
      "What is Victoria's highest monthly temperature?\n",
      "['48.8 Â°C', '48.8 Â°C', '48.8 Â°C (119.8 Â°F)']\n",
      "\n",
      "\n",
      "Victoria's highest recorded monthly temperature is 43.2Â°C (109.8Â°F), which was recorded in January 2009.\n",
      "\n",
      "How much money has been raised by the host committee?\n",
      "['$40 million', 'over $40 million', '$40 million']\n",
      "\n",
      "\n",
      "As of July 2020, the host committee for the 2020 Democratic National Convention had raised a total of $64 million.\n",
      "\n",
      "When was a patent-sharing agreement signed between Westinghouse Electric and General Electric?\n",
      "['1896', '1896', '1896']\n",
      "\n",
      "\n",
      "The patent-sharing agreement between Westinghouse Electric and General Electric was signed in 1891.\n",
      "\n",
      "Which channels did Frank Marx think would be requisitioned by the U.S. Army?\n",
      "['channels 2 through 6', '2 through 6', 'low-band VHF']\n",
      "\n",
      "\n",
      "Frank Marx believed that the U.S. Army would requisition all available television and radio channels, as well as all telephone and telegraph services.\n",
      "\n",
      "On what date did ESPN Deportes announce their deal with CBS and the NFL?\n",
      "['December 28, 2015', 'December 28, 2015', 'December 28, 2015,']\n",
      "\n",
      "\n",
      "ESPN Deportes announced their deal with CBS and the NFL on August 20, 2019.\n",
      "\n",
      "Approximately how many items are in the V&A's textiles collection?\n",
      "['more than 53,000', '53,000', '53,000']\n",
      "\n",
      "\n",
      "The V&A's textiles collection contains over 80,000 objects.\n",
      "\n",
      "How many rooms does the Warsaw Historical Museum have?\n",
      "['60', '60', '60']\n",
      "\n",
      "\n",
      "The Warsaw Historical Museum has four permanent exhibition rooms.\n",
      "\n",
      "How old are the gravestones that reference the plague?\n",
      "['dating to 1338â€“39', '1338â€“39', '1338â€“39']\n",
      "\n",
      "\n",
      "The age of the gravestones that reference the plague can vary, as some may have been placed centuries ago while others may be more recent.\n",
      "\n",
      "How many times did Arizona turn the ball over in the NFC Championship?\n",
      "['seven', 'seven', 'seven']\n",
      "\n",
      "\n",
      "Arizona turned the ball over four times in the NFC Championship.\n",
      "\n",
      "When did Luther enter into the senate of the Theology faculty of the University of Wittenberg?\n",
      "['21 October 1512', '21 October 1512', 'October 1512,']\n",
      "\n",
      "\n",
      "Martin Luther entered the senate of the Theology faculty of the University of Wittenberg in April 1512.\n",
      "\n",
      "How much gold did Victoria produce in the years of 1851-1860?\n",
      "['20 million ounces', '20 million ounces', '20 million ounces']\n",
      "\n",
      "\n",
      "Victoria produced an estimated total of 44.9 million ounces of gold during the years of 1851-1860.\n",
      "\n",
      "How many white settlers were living in Kenya in the 1950's?\n",
      "['80,000', '80,000', '80,000']\n",
      "\n",
      "\n",
      "It is difficult to provide an accurate number as there is no reliable source for this information. Estimates suggest that there were around 50,000 white settlers in Kenya in the 1950s.\n",
      "\n",
      "What is the time period represented in the museum's textiles collection?\n",
      "['from the 1st century AD to the present', '1st century AD to the present,', '1st century AD to the present']\n",
      "\n",
      "\n",
      "The time period represented in the museum's textiles collection varies depending on the specific pieces in the collection. Generally speaking, the collection includes textiles from the Middle Ages through the modern day, with some pieces dating back to the early 1500s.\n",
      "\n",
      "When did BSkyB announce it's intention to replace it's free-to-air digital channels?\n",
      "['8 February 2007', 'On 8 February 2007', '8 February 2007']\n",
      "\n",
      "\n",
      "BSkyB announced its intention to replace its free-to-air digital channels in November 2009.\n",
      "\n",
      "How many galleries does the V&A have?\n",
      "['145', '145', '145 galleries', '145']\n",
      "\n",
      "\n",
      "The Victoria and Albert Museum has over 30 permanent galleries.\n",
      "\n",
      "What 3 things does the Air Force work key on \n",
      "['use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks', 'ideas']\n",
      "\n",
      "\n",
      "1. Professionalism \n",
      "2. Integrity \n",
      "3. Excellence\n",
      "\n",
      "What is the height of the section that turns north? \n",
      "['599 m', '599 m to 396 m', '599 m to 396 m', '599 m', '599 m']\n",
      "\n",
      "\n",
      "The exact height of the section that turns north is not known. However, most sections of the Great Wall of China are thought to be anywhere between 5 and 8 meters (16.4 and 26.2 feet) high.\n",
      "\n",
      "bassett focuses on what to illustrate his idea?\n",
      "['nineteenth-century maps', 'nineteenth-century maps', 'the role of nineteenth-century maps', 'the role of nineteenth-century maps', 'the role of nineteenth-century maps during the \"scramble for Africa\"']\n",
      "\n",
      "\n",
      "He focuses on his own experiences, cultural references, and visual metaphors to illustrate his ideas. He often uses illustrations of everyday objects to show the connections between his ideas and the world around us. He also likes to draw from his own life and the world around him to create illustrations that speak to larger concepts.\n",
      "\n",
      "How many Nobel Laureates are among the school alumni?\n",
      "['150 Nobel laureates', '150', '150']\n",
      "\n",
      "\n",
      "There is no definitive answer to this question, as the number of Nobel Laureates among school alumni varies from school to school.\n",
      "\n",
      "What proportion of the general population in the area than became Iran did Genghis Khan kill?\n",
      "['three-fourths', 'three-fourths', 'up to three-fourths of the population']\n",
      "\n",
      "\n",
      "It is impossible to answer this question definitively since there is no reliable record of the total number of people killed by Genghis Khan in the area that is now Iran. Estimates of the total death toll vary widely, ranging from tens of thousands to millions.\n",
      "\n",
      "How many tons of dust remains in the air?\n",
      "['132 million tons', '132 million tons', '132 million']\n",
      "\n",
      "\n",
      "It is impossible to say exactly how many tons of dust remain in the air. Dust particles vary in size and composition, and it is difficult to measure the total amount of dust that is present in the atmosphere.\n",
      "\n",
      "What is the turbine entry temperature of a steam turbine, in degrees Celsius?\n",
      "['565', '565 Â°C', '565 Â°C']\n",
      "\n",
      "\n",
      "The turbine entry temperature of a steam turbine depends on a variety of factors, such as the type and size of the turbine, the pressure and temperature of the steam entering the turbine, and the efficiency of the turbine. Generally, the entry temperature is between 250-400Â°C (482-752Â°F).\n",
      "\n",
      "How may yards did Peyton Manning throw?\n",
      "['2,249', '2,249', '2,249']\n",
      "\n",
      "\n",
      "Peyton Manning holds a record of 71,940 career passing yards.\n",
      "\n",
      "How many seats does Victoria have in the Australian House of Representatives?\n",
      "['37', '37', '37']\n",
      "\n",
      "\n",
      "Victoria has 38 seats in the Australian House of Representatives.\n",
      "\n",
      "When did the Jin dynasty begin?\n",
      "['1115', '1115', '1115']\n",
      "\n",
      "\n",
      "The Jin dynasty began in 265 CE and ended in 420 CE.\n",
      "\n",
      "When was the paper published that the \"Millennial Northern Hemisphere temperature reconstruction\" graph was based on?\n",
      "['1999', '1999', '1999']\n",
      "\n",
      "\n",
      "The paper the \"Millennial Northern Hemisphere temperature reconstruction\" graph was based on was published in 2009.\n",
      "\n",
      "How much did Tesla spend on the injured pigeon?\n",
      "['over $2,000', 'over $2,000,', 'over $2,000']\n",
      "\n",
      "\n",
      "Tesla did not spend any money on the injured pigeon. The bird was rescued by an animal rescue organization and taken to a nearby veterinarian for treatment.\n",
      "\n",
      "How many seats are in the debating chamber?\n",
      "['131', '131', '131']\n",
      "\n",
      "\n",
      "The debating chamber of the House of Commons in the UK has 650 seats.\n",
      "\n",
      "How much of the population is Hindu?\n",
      "['around 300,000', '300,000', '300,000']\n",
      "\n",
      "\n",
      "According to the 2011 census in India, approximately 80.5% of the population is Hindu.\n",
      "\n",
      "How many membranes does Durinskia's chloroplast have?\n",
      "['up to five', 'up to five', 'five']\n",
      "\n",
      "\n",
      "Durinskia's chloroplast has two membranes.\n",
      "\n",
      "How many kilometers is Warsaw from the Carpathian Mountains?\n",
      "['about 300', '300', '300']\n",
      "\n",
      "\n",
      "Approximately 500 kilometers.\n",
      "\n",
      "How many people did Hamas kill between 2000 to 2007?\n",
      "['542', '542', '542']\n",
      "\n",
      "\n",
      "It is not possible to provide a precise answer to this question as the data concerning Hamas-related killings is incomplete and disputed. Official estimates suggest that at least 600 Palestinians and 15 Israelis were killed in violence related to Hamas between 2000 and 2007.\n",
      "\n",
      "When was the main gallery for the V&A's contemporary glass collection opened?\n",
      "['2004', '2004', '2004']\n",
      "\n",
      "\n",
      "The main gallery for the V&A's contemporary glass collection was opened in October 2017.\n",
      "\n",
      "When was the Latin version of the word Norman first recorded?\n",
      "['9th century', '9th century', '9th century']\n",
      "\n",
      "\n",
      "The Latin version of the word \"Norman\" was first recorded in the 11th century.\n",
      "\n",
      "Which episode featured the return of William Hartnell?\n",
      "['The Three Doctors', 'The Three Doctors', 'The Three Doctors']\n",
      "\n",
      "\n",
      "The Tenth Doctor special, \"The End of Time\", featured the return of William Hartnell as the First Doctor in a brief cameo.\n",
      "\n",
      "In what year was the Commission on Pan Methodist Cooperation and Union formed?\n",
      "['2000', '2000', '2000']\n",
      "\n",
      "\n",
      "The Commission on Pan Methodist Cooperation and Union was formed in 1968.\n",
      "\n",
      "How many were in Langlades expedition?\n",
      "['300 men, including French-Canadians and warriors of the Ottawa', '300', '300 men', '300', '300 men']\n",
      "\n",
      "\n",
      "There is no single answer to this question, as the size of the expedition changed throughout its history. At its peak, Langlade's expedition included up to 200 French fur traders, Native American guides, and other personnel.\n",
      "\n",
      "How many points did Carolina lead the NFL in scoring for offensive plays?\n",
      "['500', '500', '500']\n",
      "\n",
      "\n",
      "In 2020, the Carolina Panthers led the NFL in scoring for offensive plays with 484 points.\n",
      "\n",
      "What percent live below the povertly line?\n",
      "['53% of the population', '53%', '53%']\n",
      "\n",
      "\n",
      "According to the United States Census Bureau, 12.3% of people in the United States live below the poverty line.\n",
      "\n",
      "Between what dates was the building on Pilgrim Street refurbished? \n",
      "['November 2006 and May 2008', 'November 2006 and May 2008', 'between November 2006 and May 2008']\n",
      "\n",
      "\n",
      "The building on Pilgrim Street was refurbished from June 2019 to August 2020.\n",
      "\n",
      "In 2009 what was the total of Grants awarded from Harvard?\n",
      "['$414 million', '$414 million', '$414 million']\n",
      "\n",
      "\n",
      "In 2009, Harvard University awarded a total of $536 million in grants and scholarships.\n",
      "\n",
      "At what temperature will oxygen condense?\n",
      "['90.20 K', '90.20 K', '90.20 K (âˆ’182.95 Â°C, âˆ’297.31 Â°F)', '90.20 K', '90.20 K (âˆ’182.95 Â°C, âˆ’297.31 Â°F)']\n",
      "\n",
      "\n",
      "Oxygen condenses at -273.15Â°C (-459.67Â°F).\n",
      "\n",
      "Setting speed limits was one of the further devolutions which was conferred by what act?\n",
      "['2012 Act', '2012 Act', 'The 2012 Act']\n",
      "\n",
      "\n",
      "The speed limits devolution was conferred by the Transport Act 1968.\n",
      "\n",
      "About how many Walloons and Huguenots emigrated to England and Ireland in this era?\n",
      "['50,000', '50,000', '50,000']\n",
      "\n",
      "\n",
      "It is estimated that around 10,000 Walloons and Huguenots emigrated to England and Ireland during this period.\n",
      "\n",
      "How many central conferences are outside of the United States?\n",
      "['seven', 'seven central conferences: Africa, Congo, West Africa, Central & Southern Europe, Germany, Northern Europe and the Philippines.', 'seven']\n",
      "\n",
      "\n",
      "There are three central conferences outside of the United States: the Central Conferences of Africa, Europe, and the Philippines.\n",
      "\n",
      "When did Tesla get his US citizenship?\n",
      "['1891', 'On 30 July 1891', '30 July 1891']\n",
      "\n",
      "\n",
      "Tesla became a naturalized United States citizen on 18 July 1891.\n",
      "\n",
      "How large can ctenophora grow?\n",
      "['1.5 m (4 ft 11 in)', 'a few millimeters to 1.5 m']\n",
      "\n",
      "\n",
      "Ctenophora generally range in size from 0.5 millimeters (0.02 inches) to 30 centimeters (12 inches) in length.\n",
      "\n",
      "When did Li Tan lead a revolt?\n",
      "['1262', '1262', '1262']\n",
      "\n",
      "\n",
      "Li Tan led a revolt in the year 938.\n",
      "\n",
      "How many campuses does the California State University have?\n",
      "['12', '12', '12']\n",
      "\n",
      "\n",
      "The California State University system has 23 campuses.\n",
      "\n",
      "Of what form are rational primes?\n",
      "['4k + 3', '4k + 3', 'Z']\n",
      "\n",
      "\n",
      "Rational primes are numbers that cannot be expressed as the ratio of two integers. They are defined as any prime number that is not a transcendental number.\n",
      "\n",
      "What is the number of plant species in economics and social interest?\n",
      "['438,000', '438,000', '438,000', '438,000 species']\n",
      "\n",
      "\n",
      "There is no definitive answer to this question, as it depends on the specific economic and social interests involved. However, according to the World Conservation Monitoring Centre, there are currently around 393,000 known species of plants.\n",
      "\n",
      "How many campuses does the University of California have?\n",
      "['5', '5', '5']\n",
      "\n",
      "\n",
      "The University of California has 10 campuses, located in Berkeley, Davis, Irvine, Los Angeles, Merced, Riverside, San Diego, Santa Barbara, Santa Cruz, and San Francisco.\n",
      "\n",
      "What is the average construction salary in the UK?\n",
      "['Â£26,719', 'Â£26,719', 'Â£26,719']\n",
      "\n",
      "\n",
      "According to the Office for National Statistics, the average annual salary for construction and building trades in the UK in 2019 was Â£33,400.\n",
      "\n",
      "How quickly is the sea level rising?\n",
      "['1â€“3 cm (0.39â€“1.18 in) per century', '1â€“3 cm (0.39â€“1.18 in) per century', '1â€“3 cm (0.39â€“1.18 in) per century']\n",
      "\n",
      "\n",
      "The global average sea level has risen 8 inches (20 cm) since 1880, and the rate of sea level rise has increased significantly over the past two decades. Current estimates suggest that global average sea level could rise 1-4 feet (30-120 cm) by the end of the century.\n",
      "\n",
      "Pitt's plan called for what attacks?\n",
      "['three major offensive actions involving large numbers of regular troops', 'three major offensive actions', 'three major offensive actions', 'three major offensive actions', 'three major offensive actions']\n",
      "\n",
      "\n",
      "Pitt's plan called for an attack on French possessions in the West Indies, the Mediterranean, and India, as well as a naval blockade of France.\n",
      "\n",
      "When were the public housing developments built in the neighborhood?\n",
      "['between the 1960s and 1990s', 'between the 1960s and 1990s', 'between the 1960s and 1990s']\n",
      "\n",
      "\n",
      "Public housing developments in the neighborhood were built between the 1950s and the 1980s.\n",
      "\n",
      "When did Luther receive a degree in Biblical studies?\n",
      "['9 March 1508', '9 March 1508', '9 March 1508,']\n",
      "\n",
      "\n",
      "Luther received his Bachelor of Arts degree in Biblical studies in 1501.\n",
      "\n",
      "How old are the fossils found that represent ctenophhores ?\n",
      "['515 million years', '66 million years ago', '515 million years']\n",
      "\n",
      "\n",
      "The oldest known ctenophore fossils are from the Cambrian period, approximately 540 million years ago.\n",
      "\n",
      "What was the resolution of the cameras used in the EyeVision 360 system?\n",
      "['5K', '5K', '5K']\n",
      "\n",
      "\n",
      "The resolution of the cameras used in the EyeVision 360 system is 5 megapixels.\n",
      "\n",
      "When did Great Britain claim Australia? \n",
      "['1788', '1788', '1788']\n",
      "\n",
      "\n",
      "Great Britain first claimed the eastern part of Australia in 1770 when Captain James Cook, the British explorer, landed on the east coast and claimed the land for Britain.\n",
      "\n",
      "How much Saharan dust falls over the Amazon basin each year?\n",
      "['27.7 million tons', '27.7 million tons', '27.7 million tons']\n",
      "\n",
      "\n",
      "It is difficult to quantify the exact amount of Saharan dust that falls over the Amazon basin each year as the amount varies depending on factors such as wind patterns and seasonal weather patterns. However, studies have estimated that between 0.5 and 1.2 million tons of Saharan dust is carried to the Amazon basin each year.\n",
      "\n",
      "What did ABC do that was special in 2003?\n",
      "['weekly screenings of all available classic episodes', 'screenings of all available classic episodes', 'repeated episodes']\n",
      "\n",
      "\n",
      "In 2003, ABC launched its \"Magic of Disney\" campaign. The campaign showcased Disney's iconic characters and films and featured celebrity-hosted Halloween and Christmas specials. The initiative was a success and helped ABC become the highest-rated network among adults 18-49.\n",
      "\n",
      "What was the number of solo tackles that Von Miller had in Super Bowl 50?\n",
      "['5', 'five', 'five']\n",
      "\n",
      "\n",
      "Von Miller had 2 solo tackles in Super Bowl 50.\n",
      "\n",
      "How much oxygen is found is a liter of fresh water under normal conditions?\n",
      "['6.04 milliliters', '6.04 milliliters', '6.04 milliliters', '6.04 milliliters', '6.04 milliliters']\n",
      "\n",
      "\n",
      "Under normal conditions, a liter of fresh water typically contains about 8 milligrams of oxygen.\n",
      "\n",
      "When did the Jin dynasty end?\n",
      "['1234', '1234', '1234']\n",
      "\n",
      "\n",
      "The Jin dynasty ended in 420 CE.\n",
      "\n",
      "How early did Luther say he had to awaken every day?\n",
      "['at four', 'four', 'four']\n",
      "\n",
      "\n",
      "Luther said he had to awaken at 5 a.m. every day.\n",
      "\n",
      "How much gun powder was destroyed in attack?\n",
      "['45,000 pounds', '45,000 pounds', '45,000 pounds', '45,000 pounds', '45,000 pounds']\n",
      "\n",
      "\n",
      "It is not possible to answer this question without more information.\n",
      "\n",
      "In what year was the first Doctor Who audiobook released?\n",
      "['1981', '1966', '1981']\n",
      "\n",
      "\n",
      "The first Doctor Who audiobook was released in 2003.\n",
      "\n",
      "When was the Upper Rhine sold to Burgundy?\n",
      "['1469', '1469', '1469']\n",
      "\n",
      "\n",
      "The Upper Rhine was sold to Burgundy in 1349.\n",
      "\n",
      "In what year did the Apollo 1 cabin fire occur?\n",
      "['1967', '1967', '1967', '1967', '1967']\n",
      "\n",
      "\n",
      "The Apollo 1 cabin fire occurred in 1967.\n",
      "\n",
      "When did Kublai ban the international Mongol slave trade?\n",
      "['1291', '1291', '1291']\n",
      "\n",
      "\n",
      "Kublai Khan banned the international Mongol slave trade in 1260.\n",
      "\n",
      "When did the origins of magnetic and electric fields occur?\n",
      "['1864', '1864', '1864', '1864']\n",
      "\n",
      "\n",
      "The origins of magnetic and electric fields can be traced back to the beginning of the universe, approximately 13.8 billion years ago.\n",
      "\n",
      "Over how many studies have shown that violence is more common in societies with income differences?\n",
      "['fifty', 'over fifty', 'fifty']\n",
      "\n",
      "\n",
      "There have been hundreds of studies conducted on the relationship between income differences and violence. The results of these studies have been mixed, with some indicating a positive correlation between income differences and violence, while others have found no correlation.\n",
      "\n",
      "How many floors are there in the building that was completed in 1967?\n",
      "['28', '42', '42']\n",
      "\n",
      "\n",
      "The number of floors in a building depends on the building itself and so this question cannot be answered.\n",
      "\n",
      "When did Temur rule?\n",
      "['1294 to 1307', '1294 to 1307', 'from 1294 to 1307']\n",
      "\n",
      "\n",
      "Temur (or Tamerlane) ruled from 1370 to 1405.\n",
      "\n",
      "How many buildings were razed by the Jacksonville fire?\n",
      "['over 2,000', '2,000 buildings', 'over 2,']\n",
      "\n",
      "\n",
      "It is not possible to provide an exact number of buildings razed by the Jacksonville fire of 1901, as records of the fire's destruction were not kept. However, the fire destroyed most of the city's business district, including at least 145 buildings.\n",
      "\n",
      "How many registered nurses were in Kenya in 2011?\n",
      "['65,000', '65,000', '65,000']\n",
      "\n",
      "\n",
      "It is not possible to answer this question with certainty, as there is no single source of information on the total number of registered nurses in Kenya in 2011.\n",
      "\n",
      "What was the percentage of whit people in Fresno in 2010?\n",
      "['49.6%', '49.6%', '49.6%']\n",
      "\n",
      "\n",
      "According to the 2010 US Census, the population of Fresno was 49.3% White, 43.1% Hispanic or Latino, 6.2% Asian, 3.0% Black or African American, and 0.5% Native American.\n",
      "\n",
      "What omen was Genghis Khan reported to have seen assuring his coming victory against the Tanguts?\n",
      "['a line of five stars arranged in the sky', 'a line of five stars', 'a line of five stars arranged in the sky']\n",
      "\n",
      "\n",
      "Genghis Khan is said to have seen a white falcon soaring in the sky before his victory against the Tanguts. This was seen as an omen of victory and success.\n",
      "\n",
      "What yard marker on the field was painted gold?\n",
      "['50', '50-yard line', '50']\n",
      "\n",
      "\n",
      "There is no standard yard marker on a football field that is painted gold.\n",
      "\n",
      "How long is the Rhine? \n",
      "['1,230 km', '1,230 km (760 mi)', '1,230 km', '1,230 km']\n",
      "\n",
      "\n",
      "The Rhine is 1,233 km (766 miles) long.\n",
      "\n",
      "When was the first direct elections for native Kenyans?\n",
      "['1957', '1957', '1957']\n",
      "\n",
      "\n",
      "The first direct elections for native Kenyans took place in December 1963, when a new constitution was introduced allowing for the first direct elections of African representatives to the Legislative Council.\n",
      "\n",
      "In which year did the V&A received the Talbot Hughes collection?\n",
      "['1913', '1913', '1913', '1913']\n",
      "\n",
      "\n",
      "The V&A received the Talbot Hughes collection in 1932.\n",
      "\n",
      "What day of the week are general elections held?\n",
      "['Thursday', 'Thursday', 'Thursday']\n",
      "\n",
      "\n",
      "General elections in the United States are typically held on the first Tuesday of November.\n",
      "\n",
      "What was result of French attack of trading centre?\n",
      "['capturing three traders and killing 14 people of the Miami nation, including Old Briton', 'capturing three traders and killing 14 people of the Miami nation', 'capturing three traders and killing 14 people of the Miami nation, including Old Briton', 'capturing three traders and killing 14 people', 'capturing three traders and killing 14 people of the Miami nation']\n",
      "\n",
      "\n",
      "The French attack of the trading centre was ultimately unsuccessful. The French Navy had to retreat after significant losses, leaving the trading centre in British hands.\n",
      "\n",
      "What reference is there to Huguenot lacemakers in the 19th century?\n",
      "['twenty-five widows who settled in Dover', 'twenty-five widows who settled in Dover', 'twenty-five widows who settled in Dover']\n",
      "\n",
      "\n",
      "Huguenot lacemakers played an important role in the development of lace-making in the 19th century. In the late 18th and early 19th centuries, many Huguenot refugees fled to England due to religious persecution in France. This influx of skilled craftspeople served to strengthen the lace-making industry in England\n",
      "\n",
      "How many geomorphologic formations is Warsaw on?\n",
      "['two', 'two', 'two']\n",
      "\n",
      "\n",
      "Warsaw is located on three distinct geomorphologic formations. The Vistula River Valley, the Vistula Spit, and the Masovian Plain.\n",
      "\n",
      "At this time where was Luther's focus centered?\n",
      "['Daniel 8:9â€“12, 23â€“25', 'prophecy', 'prophecy of the Little Horn']\n",
      "\n",
      "\n",
      "At this time, Luther's focus was centered on reforming the Catholic Church and challenging its authority and practices.\n",
      "\n",
      "How many French colonists were gained by British?\n",
      "['80,000', '80,000', '80,000', '80,000', '80,000']\n",
      "\n",
      "\n",
      "It is impossible to determine how many French colonists were gained by the British as a result of various wars and treaties over the centuries.\n",
      "\n",
      "How many Khitan Tumens were there?\n",
      "['3', 'three']\n",
      "\n",
      "\n",
      "The Khitan people, an ethnic minority from northern China, were divided into nine tumen, or tribes, during the Liao Dynasty (907â€“1125). Each of these tumen was further divided into ten sub-tumen. In total, there were 90 Khitan tumen.\n",
      "\n",
      "The two AAA clubs divided the state into a northern and southern California as opposed to what point of view?\n",
      "['three-region', 'the three-region point of view', 'three-region']\n",
      "\n",
      "\n",
      "Geographic.\n",
      "\n",
      "When did Luther save the group of nuns from the convent?\n",
      "['April 1523', 'April 1523', '1523']\n",
      "\n",
      "\n",
      "Martin Luther saved the group of nuns from the convent in 1522 during the German Peasants' War.\n",
      "\n",
      "How high was the stone wall built around Newcastle in the 13th century?\n",
      "['25-foot', '25-foot', '25-foot (7.6 m) high']\n",
      "\n",
      "\n",
      "The stone wall around Newcastle was built to a height of 12-15 feet in the 13th century.\n",
      "\n",
      "What percentage of Scotland's voting population failed to actually vote?\n",
      "['32.9%', '32.9%', '32.9%']\n",
      "\n",
      "\n",
      "In the 2019 UK General Election, 28.6% of eligible voters in Scotland did not vote.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in sq_num_zero:\n",
    "    if d['human']==0:\n",
    "        print(d['question'])\n",
    "        print(d['answers'])\n",
    "        print(d['prediction'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f89c0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce442435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human evaluation:  0.44\n",
      "Soft EM evaluation:  0.33\n",
      "Hard EM evaluation:  0.03\n",
      "F1 score:  0.16\n"
     ]
    }
   ],
   "source": [
    "human_sum=0\n",
    "soft_em_sum=0\n",
    "hard_em_sum=0\n",
    "f1_sum=0\n",
    "\n",
    "for d in sq_non_zero:\n",
    "    human_sum+=d['human']\n",
    "    soft_em_sum+=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answers'])\n",
    "    hard_em_sum+=metric_max_over_ground_truths(hard_exact_match_score, d['prediction'], d['answers'])\n",
    "    f1_sum+=metric_max_over_ground_truths(f1_score, d['prediction'], d['answers'])\n",
    "\n",
    "print('Human evaluation: %5.2f' % (human_sum/len(nq_num_zero)))\n",
    "print('Soft EM evaluation: %5.2f' % (soft_em_sum/len(nq_num_zero)))\n",
    "print('Hard EM evaluation: %5.2f' % (hard_em_sum/len(nq_num_zero)))\n",
    "print('F1 score: %5.2f' % (f1_sum/len(nq_num_zero)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ad7426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human evaluation:  0.31\n",
      "Soft EM evaluation:  0.23\n",
      "Hard EM evaluation:  0.19\n",
      "F1 score:  0.33\n"
     ]
    }
   ],
   "source": [
    "human_sum=0\n",
    "soft_em_sum=0\n",
    "hard_em_sum=0\n",
    "f1_sum=0\n",
    "\n",
    "for d in sq_num_few:\n",
    "    human_sum+=d['human']\n",
    "    soft_em_sum+=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answers'])\n",
    "    hard_em_sum+=metric_max_over_ground_truths(hard_exact_match_score, d['prediction'], d['answers'])\n",
    "    f1_sum+=metric_max_over_ground_truths(f1_score, d['prediction'], d['answers'])\n",
    "\n",
    "print('Human evaluation: %5.2f' % (human_sum/len(nq_num_zero)))\n",
    "print('Soft EM evaluation: %5.2f' % (soft_em_sum/len(nq_num_zero)))\n",
    "print('Hard EM evaluation: %5.2f' % (hard_em_sum/len(nq_num_zero)))\n",
    "print('F1 score: %5.2f' % (f1_sum/len(nq_num_zero)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b99ef676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human evaluation:  0.43\n",
      "Soft EM evaluation:  0.32\n",
      "Hard EM evaluation:  0.23\n",
      "F1 score:  0.38\n"
     ]
    }
   ],
   "source": [
    "human_sum=0\n",
    "soft_em_sum=0\n",
    "hard_em_sum=0\n",
    "f1_sum=0\n",
    "\n",
    "for d in sq_non_few:\n",
    "    human_sum+=d['human']\n",
    "    soft_em_sum+=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answers'])\n",
    "    hard_em_sum+=metric_max_over_ground_truths(hard_exact_match_score, d['prediction'], d['answers'])\n",
    "    f1_sum+=metric_max_over_ground_truths(f1_score, d['prediction'], d['answers'])\n",
    "\n",
    "print('Human evaluation: %5.2f' % (human_sum/len(nq_num_zero)))\n",
    "print('Soft EM evaluation: %5.2f' % (soft_em_sum/len(nq_num_zero)))\n",
    "print('Hard EM evaluation: %5.2f' % (hard_em_sum/len(nq_num_zero)))\n",
    "print('F1 score: %5.2f' % (f1_sum/len(nq_num_zero)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6364584f",
   "metadata": {},
   "source": [
    "### Numeric BEM-NEM(Ans)-NEM(Sen)-Human"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805cb22e",
   "metadata": {},
   "source": [
    "### NQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ec748a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in nq_num_zero:\n",
    "    bem=bem_score(d['prediction'], d['answer'], d['question'])\n",
    "    num_ans=num_ans_score(d['prediction'], d['answer'], d['question'])\n",
    "    num_sen=num_sen_score(d['prediction'], d['answer'], d['question'])\n",
    "    num_integ=num_integ_score(d['prediction'], d['answer'], d['question'])\n",
    "    soft_em=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answer'])\n",
    "    hard_em=metric_max_over_ground_truths(hard_exact_match_score, d['prediction'], d['answer'])\n",
    "    d['bem']=bem\n",
    "    d['num_ans']=num_ans\n",
    "    d['num_sen']=num_sen\n",
    "    d['num_integ']=num_integ\n",
    "    d['soft_em']=soft_em\n",
    "    d['hard_em']=hard_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54ccf2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NQ 150 Samples InstGPT zero-shot\n",
      "BEM accuracy:  0.83\n",
      "NEM_ans accuracy:  0.62\n",
      "NEM_sen accuracy:  0.45\n",
      "NEM_integ accuracy:  0.47\n",
      "Human accuracy:  0.53\n",
      "\n",
      "BEM accuracy against human label:  0.67\n",
      "NEM_ans accuracy against human label:  0.71\n",
      "NEM_sen accuracy against human label:  0.73\n",
      "NEM_integ accuracy against human label:  0.72\n"
     ]
    }
   ],
   "source": [
    "bem_human=0\n",
    "num_ans_human=0\n",
    "num_sen_human=0\n",
    "num_integ_human=0\n",
    "human_sum=0\n",
    "bem_sum=0\n",
    "num_ans_sum=0\n",
    "num_sen_sum=0\n",
    "num_integ_sum=0\n",
    "\n",
    "for d in nq_num_zero:\n",
    "    if d['bem']==d['human']:\n",
    "        bem_human+=1\n",
    "    if d['num_ans']==d['human']:\n",
    "        num_ans_human+=1\n",
    "    if d['num_sen']==d['human']:\n",
    "        num_sen_human+=1\n",
    "    if d['num_integ']==d['human']:\n",
    "        num_integ_human+=1\n",
    "        \n",
    "    human_sum+=d['human']\n",
    "    bem_sum+=d['bem']\n",
    "    num_ans_sum+=d['num_ans']\n",
    "    num_sen_sum+=d['num_sen']\n",
    "    num_integ_sum+=d['num_integ']\n",
    "\n",
    "print('NQ 150 Samples InstGPT zero-shot')\n",
    "print('BEM accuracy: %5.2f'%(bem_sum/150))\n",
    "print('NEM_ans accuracy: %5.2f'%(num_ans_sum/150))\n",
    "print('NEM_sen accuracy: %5.2f'%(num_sen_sum/150))\n",
    "print('NEM_integ accuracy: %5.2f'%(num_integ_sum/150))\n",
    "print('Human accuracy: %5.2f' %(human_sum/150))\n",
    "print()\n",
    "print('BEM accuracy against human label: %5.2f'%(bem_human/150))\n",
    "print('NEM_ans accuracy against human label: %5.2f'%(num_ans_human/150))\n",
    "print('NEM_sen accuracy against human label: %5.2f'%(num_sen_human/150))\n",
    "print('NEM_integ accuracy against human label: %5.2f'%(num_integ_human/150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "766c07ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in nq_num_few:\n",
    "    bem=bem_score(d['prediction'], d['answer'], d['question'])\n",
    "    num_ans=num_ans_score(d['prediction'], d['answer'], d['question'])\n",
    "    num_sen=num_sen_score(d['prediction'], d['answer'], d['question'])\n",
    "    num_integ=num_integ_score(d['prediction'], d['answer'], d['question'])\n",
    "    soft_em=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answer'])\n",
    "    hard_em=metric_max_over_ground_truths(hard_exact_match_score, d['prediction'], d['answer'])\n",
    "    d['bem']=bem\n",
    "    d['num_ans']=num_ans\n",
    "    d['num_sen']=num_sen\n",
    "    d['num_integ']=num_integ\n",
    "    d['soft_em']=soft_em\n",
    "    d['hard_em']=hard_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "626cd5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NQ 150 Samples InstGPT few-shot\n",
      "BEM accuracy:  0.69\n",
      "NEM_ans accuracy:  0.53\n",
      "NEM_sen accuracy:  0.97\n",
      "NEM_integ accuracy:  0.51\n",
      "Human accuracy:  0.56\n",
      "\n",
      "BEM accuracy against human label:  0.81\n",
      "NEM_ans accuracy against human label:  0.89\n",
      "NEM_sen accuracy against human label:  0.57\n",
      "NEM_integ accuracy against human label:  0.90\n"
     ]
    }
   ],
   "source": [
    "bem_human=0\n",
    "num_ans_human=0\n",
    "num_sen_human=0\n",
    "num_integ_human=0\n",
    "human_sum=0\n",
    "bem_sum=0\n",
    "num_ans_sum=0\n",
    "num_sen_sum=0\n",
    "num_integ_sum=0\n",
    "\n",
    "for d in nq_num_few:\n",
    "    if d['bem']==d['human']:\n",
    "        bem_human+=1\n",
    "    if d['num_ans']==d['human']:\n",
    "        num_ans_human+=1\n",
    "    if d['num_sen']==d['human']:\n",
    "        num_sen_human+=1\n",
    "    if d['num_integ']==d['human']:\n",
    "        num_integ_human+=1\n",
    "        \n",
    "    human_sum+=d['human']\n",
    "    bem_sum+=d['bem']\n",
    "    num_ans_sum+=d['num_ans']\n",
    "    num_sen_sum+=d['num_sen']\n",
    "    num_integ_sum+=d['num_integ']\n",
    "\n",
    "print('NQ 150 Samples InstGPT few-shot')\n",
    "print('BEM accuracy: %5.2f'%(bem_sum/150))\n",
    "print('NEM_ans accuracy: %5.2f'%(num_ans_sum/150))\n",
    "print('NEM_sen accuracy: %5.2f'%(num_sen_sum/150))\n",
    "print('NEM_integ accuracy: %5.2f'%(num_integ_sum/150))\n",
    "print('Human accuracy: %5.2f' %(human_sum/150))\n",
    "print()\n",
    "print('BEM accuracy against human label: %5.2f'%(bem_human/150))\n",
    "print('NEM_ans accuracy against human label: %5.2f'%(num_ans_human/150))\n",
    "print('NEM_sen accuracy against human label: %5.2f'%(num_sen_human/150))\n",
    "print('NEM_integ accuracy against human label: %5.2f'%(num_integ_human/150))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0589f425",
   "metadata": {},
   "source": [
    "### SQUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4e7107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in sq_num_zero:\n",
    "    bem=bem_score(d['prediction'], d['answers'], d['question'])\n",
    "    num_ans=num_ans_score(d['prediction'], d['answers'], d['question'])\n",
    "    num_sen=num_sen_score(d['prediction'], d['answers'], d['question'])\n",
    "    num_integ=num_integ_score(d['prediction'], d['answers'], d['question'])\n",
    "    soft_em=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answers'])\n",
    "    hard_em=metric_max_over_ground_truths(hard_exact_match_score, d['prediction'], d['answers'])\n",
    "    d['bem']=bem\n",
    "    d['num_ans']=num_ans\n",
    "    d['num_sen']=num_sen\n",
    "    d['num_integ']=num_integ\n",
    "    d['soft_em']=soft_em\n",
    "    d['hard_em']=hard_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b12377c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQUAD 150 Samples InstGPT zero-shot\n",
      "BEM accuracy:  0.71\n",
      "NEM_ans accuracy:  0.57\n",
      "NEM_sen accuracy:  0.51\n",
      "NEM_integ accuracy:  0.52\n",
      "Human accuracy:  0.26\n",
      "\n",
      "BEM accuracy against human label:  0.53\n",
      "NEM_ans accuracy against human label:  0.62\n",
      "NEM_sen accuracy against human label:  0.60\n",
      "NEM_integ accuracy against human label:  0.59\n"
     ]
    }
   ],
   "source": [
    "bem_human=0\n",
    "num_ans_human=0\n",
    "num_sen_human=0\n",
    "num_integ_human=0\n",
    "human_sum=0\n",
    "bem_sum=0\n",
    "num_ans_sum=0\n",
    "num_sen_sum=0\n",
    "num_integ_sum=0\n",
    "\n",
    "for d in sq_num_zero:\n",
    "    if d['bem']==d['human']:\n",
    "        bem_human+=1\n",
    "    if d['num_ans']==d['human']:\n",
    "        num_ans_human+=1\n",
    "    if d['num_sen']==d['human']:\n",
    "        num_sen_human+=1\n",
    "    if d['num_integ']==d['human']:\n",
    "        num_integ_human+=1\n",
    "        \n",
    "    human_sum+=d['human']\n",
    "    bem_sum+=d['bem']\n",
    "    num_ans_sum+=d['num_ans']\n",
    "    num_sen_sum+=d['num_sen']\n",
    "    num_integ_sum+=d['num_integ']\n",
    "\n",
    "print('SQUAD 150 Samples InstGPT zero-shot')\n",
    "print('BEM accuracy: %5.2f'%(bem_sum/150))\n",
    "print('NEM_ans accuracy: %5.2f'%(num_ans_sum/150))\n",
    "print('NEM_sen accuracy: %5.2f'%(num_sen_sum/150))\n",
    "print('NEM_integ accuracy: %5.2f'%(num_integ_sum/150))\n",
    "print('Human accuracy: %5.2f' %(human_sum/150))\n",
    "print()\n",
    "print('BEM accuracy against human label: %5.2f'%(bem_human/150))\n",
    "print('NEM_ans accuracy against human label: %5.2f'%(num_ans_human/150))\n",
    "print('NEM_sen accuracy against human label: %5.2f'%(num_sen_human/150))\n",
    "print('NEM_integ accuracy against human label: %5.2f'%(num_integ_human/150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79c24aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in sq_num_few:\n",
    "    bem=bem_score(d['prediction'], d['answers'], d['question'])\n",
    "    num_ans=num_ans_score(d['prediction'], d['answers'], d['question'])\n",
    "    num_sen=num_sen_score(d['prediction'], d['answers'], d['question'])\n",
    "    num_integ=num_integ_score(d['prediction'], d['answers'], d['question'])\n",
    "    soft_em=metric_max_over_ground_truths(soft_exact_match_score, d['prediction'], d['answers'])\n",
    "    hard_em=metric_max_over_ground_truths(hard_exact_match_score, d['prediction'], d['answers'])\n",
    "    d['bem']=bem\n",
    "    d['num_ans']=num_ans\n",
    "    d['num_sen']=num_sen\n",
    "    d['num_integ']=num_integ\n",
    "    d['soft_em']=soft_em\n",
    "    d['hard_em']=hard_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d753ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQUAD 150 Samples InstGPT few-shot\n",
      "BEM accuracy:  0.50\n",
      "NEM_ans accuracy:  0.55\n",
      "NEM_sen accuracy:  0.97\n",
      "NEM_integ accuracy:  0.50\n",
      "Human accuracy:  0.31\n",
      "\n",
      "BEM accuracy against human label:  0.76\n",
      "NEM_ans accuracy against human label:  0.69\n",
      "NEM_sen accuracy against human label:  0.31\n",
      "NEM_integ accuracy against human label:  0.76\n"
     ]
    }
   ],
   "source": [
    "bem_human=0\n",
    "num_ans_human=0\n",
    "num_sen_human=0\n",
    "num_integ_human=0\n",
    "human_sum=0\n",
    "bem_sum=0\n",
    "num_ans_sum=0\n",
    "num_sen_sum=0\n",
    "num_integ_sum=0\n",
    "\n",
    "for d in sq_num_few:\n",
    "    if d['bem']==d['human']:\n",
    "        bem_human+=1\n",
    "    if d['num_ans']==d['human']:\n",
    "        num_ans_human+=1\n",
    "    if d['num_sen']==d['human']:\n",
    "        num_sen_human+=1\n",
    "    if d['num_integ']==d['human']:\n",
    "        num_integ_human+=1\n",
    "        \n",
    "    human_sum+=d['human']\n",
    "    bem_sum+=d['bem']\n",
    "    num_ans_sum+=d['num_ans']\n",
    "    num_sen_sum+=d['num_sen']\n",
    "    num_integ_sum+=d['num_integ']\n",
    "\n",
    "print('SQUAD 150 Samples InstGPT few-shot')\n",
    "print('BEM accuracy: %5.2f'%(bem_sum/150))\n",
    "print('NEM_ans accuracy: %5.2f'%(num_ans_sum/150))\n",
    "print('NEM_sen accuracy: %5.2f'%(num_sen_sum/150))\n",
    "print('NEM_integ accuracy: %5.2f'%(num_integ_sum/150))\n",
    "print('Human accuracy: %5.2f' %(human_sum/150))\n",
    "print()\n",
    "print('BEM accuracy against human label: %5.2f'%(bem_human/150))\n",
    "print('NEM_ans accuracy against human label: %5.2f'%(num_ans_human/150))\n",
    "print('NEM_sen accuracy against human label: %5.2f'%(num_sen_human/150))\n",
    "print('NEM_integ accuracy against human label: %5.2f'%(num_integ_human/150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adcbbe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the weight of a bushel of coal in pounds?\n",
      "['94', '94 pounds', '94 pounds']\n",
      " between 50 and 70 pounds\n",
      "\n",
      "What provided for the creation of new orders known as \"provisional elder?\"\n",
      "['1996 General Conference', '1996 General Conference the', 'the ordination order of transitional deacon was abolished']\n",
      " The Second Epistle of John\n",
      "\n",
      "For which show did Billie Piper tape an introduction?\n",
      "['The Christmas Invasion', 'The Christmas Invasion', 'The Christmas Invasion']\n",
      " Doctor Who Confidential\n",
      "\n",
      "Approximately how many items are in the V&A's textiles collection?\n",
      "['more than 53,000', '53,000', '53,000']\n",
      " 70,000\n",
      "\n",
      "How old are the gravestones that reference the plague?\n",
      "['dating to 1338â€“39', '1338â€“39', '1338â€“39']\n",
      " 17th century\n",
      "\n",
      "How much gold did Victoria produce in the years of 1851-1860?\n",
      "['20 million ounces', '20 million ounces', '20 million ounces']\n",
      " over 650 tons\n",
      "\n",
      "What is the time period represented in the museum's textiles collection?\n",
      "['from the 1st century AD to the present', '1st century AD to the present,', '1st century AD to the present']\n",
      " 6th century CE to the 21st century\n",
      "\n",
      "What 3 things does the Air Force work key on \n",
      "['use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks', 'ideas']\n",
      " mission, people, and innovation.\n",
      "\n",
      "What was the population of Kenya in 2014?\n",
      "['45 million people', '45 million people', 'approximately 45 million']\n",
      " 48.46 million\n",
      "\n",
      "What is the height of the section that turns north? \n",
      "['599 m', '599 m to 396 m', '599 m to 396 m', '599 m', '599 m']\n",
      " 8,000 feet (2,400 m)\n",
      "\n",
      "bassett focuses on what to illustrate his idea?\n",
      "['nineteenth-century maps', 'nineteenth-century maps', 'the role of nineteenth-century maps', 'the role of nineteenth-century maps', 'the role of nineteenth-century maps during the \"scramble for Africa\"']\n",
      " the human brain\n",
      "\n",
      "What proportion of the general population in the area than became Iran did Genghis Khan kill?\n",
      "['three-fourths', 'three-fourths', 'up to three-fourths of the population']\n",
      " 20-60%\n",
      "\n",
      "What is the turbine entry temperature of a steam turbine, in degrees Celsius?\n",
      "['565', '565 Â°C', '565 Â°C']\n",
      " Between 420 and 540 degrees Celsius\n",
      "\n",
      "How much did Tesla spend on the injured pigeon?\n",
      "['over $2,000', 'over $2,000,', 'over $2,000']\n",
      " $2\n",
      "\n",
      "How many kilometers is Warsaw from the Carpathian Mountains?\n",
      "['about 300', '300', '300']\n",
      " approximately 250 km\n",
      "\n",
      "How many people did Hamas kill between 2000 to 2007?\n",
      "['542', '542', '542']\n",
      " 1,164\n",
      "\n",
      "How large can ctenophora grow?\n",
      "['1.5 m (4 ft 11 in)', 'a few millimeters to 1.5 m']\n",
      " up to 20 cm in diameter\n",
      "\n",
      "How quickly is the sea level rising?\n",
      "['1â€“3 cm (0.39â€“1.18 in) per century', '1â€“3 cm (0.39â€“1.18 in) per century', '1â€“3 cm (0.39â€“1.18 in) per century']\n",
      " about 3.2 millimeters per year\n",
      "\n",
      "Pitt's plan called for what attacks?\n",
      "['three major offensive actions involving large numbers of regular troops', 'three major offensive actions', 'three major offensive actions', 'three major offensive actions', 'three major offensive actions']\n",
      " amphibious landings\n",
      "\n",
      "When were the public housing developments built in the neighborhood?\n",
      "['between the 1960s and 1990s', 'between the 1960s and 1990s', 'between the 1960s and 1990s']\n",
      " 1950s and 1960s\n",
      "\n",
      "How old are the fossils found that represent ctenophhores ?\n",
      "['515 million years', '66 million years ago', '515 million years']\n",
      " 500 million years old\n",
      "\n",
      "What was the resolution of the cameras used in the EyeVision 360 system?\n",
      "['5K', '5K', '5K']\n",
      " 1.2 megapixel resolution\n",
      "\n",
      "What did ABC do that was special in 2003?\n",
      "['weekly screenings of all available classic episodes', 'screenings of all available classic episodes', 'repeated episodes']\n",
      " aired its first Super Bowl Halftime Show\n",
      "\n",
      "When did the formation of the Holocene Rhine-Meuse delta begin?\n",
      "['8,000 years ago', '~8,000 years ago', '~8,000 years ago']\n",
      " 8,500 years ago\n",
      "\n",
      "How much oxygen is found is a liter of fresh water under normal conditions?\n",
      "['6.04 milliliters', '6.04 milliliters', '6.04 milliliters', '6.04 milliliters', '6.04 milliliters']\n",
      " 8â€“14 mg/L\n",
      "\n",
      "When did the origins of magnetic and electric fields occur?\n",
      "['1864', '1864', '1864', '1864']\n",
      " 4.6 billion years ago\n",
      "\n",
      "Over how many studies have shown that violence is more common in societies with income differences?\n",
      "['fifty', 'over fifty', 'fifty']\n",
      " over 700\n",
      "\n",
      "How many buildings were razed by the Jacksonville fire?\n",
      "['over 2,000', '2,000 buildings', 'over 2,']\n",
      " over 2,500\n",
      "\n",
      "What omen was Genghis Khan reported to have seen assuring his coming victory against the Tanguts?\n",
      "['a line of five stars arranged in the sky', 'a line of five stars', 'a line of five stars arranged in the sky']\n",
      " a white falcon\n",
      "\n",
      "What was result of French attack of trading centre?\n",
      "['capturing three traders and killing 14 people of the Miami nation, including Old Briton', 'capturing three traders and killing 14 people of the Miami nation', 'capturing three traders and killing 14 people of the Miami nation, including Old Briton', 'capturing three traders and killing 14 people', 'capturing three traders and killing 14 people of the Miami nation']\n",
      " The French were repelled by a small but determined Dutch and English garrison.\n",
      "\n",
      "At this time where was Luther's focus centered?\n",
      "['Daniel 8:9â€“12, 23â€“25', 'prophecy', 'prophecy of the Little Horn']\n",
      " the Bible and the Christian life\n",
      "\n",
      "The two AAA clubs divided the state into a northern and southern California as opposed to what point of view?\n",
      "['three-region', 'the three-region point of view', 'three-region']\n",
      " geographical\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in sq_num_few:\n",
    "    if d['human']==0:\n",
    "        if d['human']!=d['num_integ']:\n",
    "            print(d['question'])\n",
    "            print(d['answers'])\n",
    "            print(d['prediction'])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1a3699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda:.conda-numeric]",
   "language": "python",
   "name": "conda-env-.conda-numeric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
